{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDTuGD_YhYMa",
        "outputId": "dcfac4da-8ab4-419e-d2de-99de8497c5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "ASF-DCL: Action State Flow with Dynamics-aware Contrastive Learning\n",
            "--------------------------------------------------------------------------------\n",
            "Batch size: 64\n",
            "Epochs: 50\n",
            "Learning rate: 0.001\n",
            "\n",
            "Lambda_dyn: 0.05\n",
            "Lambda_flow:  0.02\n",
            "Lambda_proto: 0.05\n",
            "Lambda_contrast: 0.2\n",
            "\n",
            "[MotionSenseDataset] windows: 22053, classes: 6\n",
            "Classes map: {'dws': 0, 'jog': 1, 'sit': 2, 'std': 3, 'ups': 4, 'wlk': 5}\n",
            "Train size: 17642, Test size: 4411\n",
            "\n",
            "Total parameters: 95,462\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TRAINING\n",
            "--------------------------------------------------------------------------------\n",
            "[01/50] Train Loss: 0.854 | F1: 0.7359  |  Test F1: 0.8576 (Best: 0.8576)\n",
            "[02/50] Train Loss: 0.440 | F1: 0.9445  |  Test F1: 0.8188 (Best: 0.8576)\n",
            "[03/50] Train Loss: 0.376 | F1: 0.9662  |  Test F1: 0.7801 (Best: 0.8576)\n",
            "[04/50] Train Loss: 0.353 | F1: 0.9751  |  Test F1: 0.9332 (Best: 0.9332)\n",
            "[05/50] Train Loss: 0.327 | F1: 0.9832  |  Test F1: 0.7823 (Best: 0.9332)\n",
            "[06/50] Train Loss: 0.322 | F1: 0.9833  |  Test F1: 0.9833 (Best: 0.9833)\n",
            "[07/50] Train Loss: 0.312 | F1: 0.9861  |  Test F1: 0.8182 (Best: 0.9833)\n",
            "[08/50] Train Loss: 0.302 | F1: 0.9894  |  Test F1: 0.9000 (Best: 0.9833)\n",
            "[09/50] Train Loss: 0.301 | F1: 0.9891  |  Test F1: 0.9562 (Best: 0.9833)\n",
            "[10/50] Train Loss: 0.295 | F1: 0.9911  |  Test F1: 0.7054 (Best: 0.9833)\n",
            "[11/50] Train Loss: 0.293 | F1: 0.9917  |  Test F1: 0.9752 (Best: 0.9833)\n",
            "[12/50] Train Loss: 0.289 | F1: 0.9939  |  Test F1: 0.9557 (Best: 0.9833)\n",
            "[13/50] Train Loss: 0.283 | F1: 0.9951  |  Test F1: 0.9138 (Best: 0.9833)\n",
            "[14/50] Train Loss: 0.285 | F1: 0.9939  |  Test F1: 0.7945 (Best: 0.9833)\n",
            "[15/50] Train Loss: 0.288 | F1: 0.9930  |  Test F1: 0.9437 (Best: 0.9833)\n",
            "[16/50] Train Loss: 0.280 | F1: 0.9960  |  Test F1: 0.9500 (Best: 0.9833)\n",
            "[17/50] Train Loss: 0.280 | F1: 0.9963  |  Test F1: 0.8306 (Best: 0.9833)\n",
            "[18/50] Train Loss: 0.275 | F1: 0.9973  |  Test F1: 0.9651 (Best: 0.9833)\n",
            "[19/50] Train Loss: 0.278 | F1: 0.9968  |  Test F1: 0.9488 (Best: 0.9833)\n",
            "[20/50] Train Loss: 0.279 | F1: 0.9959  |  Test F1: 0.9522 (Best: 0.9833)\n",
            "[21/50] Train Loss: 0.277 | F1: 0.9965  |  Test F1: 0.9374 (Best: 0.9833)\n",
            "[22/50] Train Loss: 0.275 | F1: 0.9980  |  Test F1: 0.9794 (Best: 0.9833)\n",
            "[23/50] Train Loss: 0.275 | F1: 0.9975  |  Test F1: 0.9675 (Best: 0.9833)\n",
            "[24/50] Train Loss: 0.273 | F1: 0.9981  |  Test F1: 0.9758 (Best: 0.9833)\n",
            "[25/50] Train Loss: 0.275 | F1: 0.9969  |  Test F1: 0.9846 (Best: 0.9846)\n",
            "[26/50] Train Loss: 0.271 | F1: 0.9986  |  Test F1: 0.9773 (Best: 0.9846)\n",
            "[27/50] Train Loss: 0.271 | F1: 0.9989  |  Test F1: 0.9817 (Best: 0.9846)\n",
            "[28/50] Train Loss: 0.271 | F1: 0.9988  |  Test F1: 0.9909 (Best: 0.9909)\n",
            "[29/50] Train Loss: 0.271 | F1: 0.9988  |  Test F1: 0.9886 (Best: 0.9909)\n",
            "[30/50] Train Loss: 0.269 | F1: 0.9990  |  Test F1: 0.9795 (Best: 0.9909)\n",
            "[31/50] Train Loss: 0.270 | F1: 0.9990  |  Test F1: 0.9913 (Best: 0.9913)\n",
            "[32/50] Train Loss: 0.270 | F1: 0.9989  |  Test F1: 0.9828 (Best: 0.9913)\n",
            "[33/50] Train Loss: 0.269 | F1: 0.9993  |  Test F1: 0.9874 (Best: 0.9913)\n",
            "[34/50] Train Loss: 0.268 | F1: 0.9991  |  Test F1: 0.9932 (Best: 0.9932)\n",
            "[35/50] Train Loss: 0.269 | F1: 0.9994  |  Test F1: 0.9889 (Best: 0.9932)\n",
            "[36/50] Train Loss: 0.268 | F1: 0.9998  |  Test F1: 0.9946 (Best: 0.9946)\n",
            "[37/50] Train Loss: 0.267 | F1: 0.9996  |  Test F1: 0.9938 (Best: 0.9946)\n",
            "[38/50] Train Loss: 0.268 | F1: 0.9994  |  Test F1: 0.9924 (Best: 0.9946)\n",
            "[39/50] Train Loss: 0.267 | F1: 0.9998  |  Test F1: 0.9713 (Best: 0.9946)\n",
            "[40/50] Train Loss: 0.267 | F1: 0.9998  |  Test F1: 0.9938 (Best: 0.9946)\n",
            "[41/50] Train Loss: 0.267 | F1: 0.9999  |  Test F1: 0.9931 (Best: 0.9946)\n",
            "[42/50] Train Loss: 0.267 | F1: 0.9999  |  Test F1: 0.9943 (Best: 0.9946)\n",
            "[43/50] Train Loss: 0.267 | F1: 0.9998  |  Test F1: 0.9949 (Best: 0.9949)\n",
            "[44/50] Train Loss: 0.267 | F1: 0.9997  |  Test F1: 0.9948 (Best: 0.9949)\n",
            "[45/50] Train Loss: 0.266 | F1: 0.9999  |  Test F1: 0.9944 (Best: 0.9949)\n",
            "[46/50] Train Loss: 0.267 | F1: 0.9997  |  Test F1: 0.9951 (Best: 0.9951)\n",
            "[47/50] Train Loss: 0.267 | F1: 0.9999  |  Test F1: 0.9939 (Best: 0.9951)\n",
            "[48/50] Train Loss: 0.266 | F1: 0.9999  |  Test F1: 0.9953 (Best: 0.9953)\n",
            "[49/50] Train Loss: 0.267 | F1: 0.9999  |  Test F1: 0.9955 (Best: 0.9955)\n",
            "[50/50] Train Loss: 0.266 | F1: 1.0000  |  Test F1: 0.9955 (Best: 0.9955)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "EVALUATION...\n",
            "--------------------------------------------------------------------------------\n",
            "Final Result â†’ Best Test F1: 0.9955 (Acc: 0.9961)\n",
            "\n",
            "================================================================================\n",
            " ğŸ§ª HYPOTHESIS VERIFICATION EXPERIMENTS\n",
            "================================================================================\n",
            "\n",
            "[Experiment 1] Flow Magnitude Analysis (Sensitivity Check)\n",
            "------------------------------------------------------------\n",
            "Condition       | Avg Flow (Static)    | Avg Flow (Dynamic)  \n",
            "------------------------------------------------------------\n",
            "Bias 0.0        | 0.142368             | 0.680424\n",
            "Bias 0.1        | 0.141742 (+-0.0006)  | 0.683556\n",
            "------------------------------------------------------------\n",
            ">> ì¦ëª… í¬ì¸íŠ¸: Bias 0.1ì¼ ë•Œ Static Flow ê°’ì´ ê¸‰ê²©íˆ ìƒìŠ¹í–ˆëŠ”ê°€? (YESì—¬ì•¼ í•¨)\n",
            "\n",
            "[Experiment 2] Confusion Matrix Pattern (Misclassification Check)\n",
            "------------------------------------------------------------\n",
            "\n",
            "< Confusion Matrix (Bias = 0.0) >\n",
            "     dws  jog   sit  std  ups   wlk\n",
            "dws  432    0     0    0    3     1\n",
            "jog    0  395     0    0    0     0\n",
            "sit    0    0  1047    1    0     0\n",
            "std    0    0     0  956    0     0\n",
            "ups    0    0     0    0  496     6\n",
            "wlk    1    0     0    0    5  1068\n",
            "\n",
            "Accuracy: 0.9961\n",
            "\n",
            "< Confusion Matrix (Bias = 0.1) >\n",
            "     dws  jog  sit  std  ups   wlk\n",
            "dws  417    0    0    0    9    10\n",
            "jog    0  395    0    0    0     0\n",
            "sit    0    0  514  534    0     0\n",
            "std    0    0    0  956    0     0\n",
            "ups    0    0    0    0  489    13\n",
            "wlk    0    0    0    0    4  1070\n",
            "\n",
            "Accuracy: 0.8708\n",
            "------------------------------------------------------------\n",
            ">> ì¦ëª… í¬ì¸íŠ¸: Bias 0.1ì—ì„œ 'sit', 'std' í–‰(Row)ì´ 'wlk', 'jog' ì—´(Column)ë¡œ ì´ë™í–ˆëŠ”ê°€?\n",
            ">> ì¦‰, Static -> Dynamic ì˜¤ë¶„ë¥˜ê°€ ë°œìƒí–ˆëŠ”ê°€?\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. motion-sense Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class MotionSenseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Motion-Sense Dataset Loader\n",
        "    - ì…ë ¥: (T, 6) -> (userAcceleration.x, y, z, rotationRate.x, y, z)\n",
        "    - ë¼ë²¨: í´ë”ëª…(dws, ups, wlk, jog, sit, std)ì„ íŒŒì‹±í•˜ì—¬ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir,\n",
        "        window_size=128,\n",
        "        step_size=64,\n",
        "        normalize=True,\n",
        "        target_subjects=None,\n",
        "        scaler=None\n",
        "    ):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        self.normalize = normalize\n",
        "\n",
        "        # Motion-Senseì˜ ë°ì´í„° í´ë” ê²½ë¡œ (ì´ë¯¸ì§€ ê¸°ì¤€ A_DeviceMotion_data í´ë”)\n",
        "        self.data_dir = self.root_dir / \"A_DeviceMotion_data\"\n",
        "\n",
        "        # 1) ë°ì´í„° ë¡œë“œ ë° í†µí•©\n",
        "        df_all = self._load_all_data()\n",
        "\n",
        "        if target_subjects is not None:\n",
        "            df_all = df_all[df_all['subject_id'].isin(target_subjects)].copy()\n",
        "            print(f\"Dataset initialized with subjects: {target_subjects}\")\n",
        "            print(f\"Total rows after filtering: {len(df_all)}\")\n",
        "\n",
        "        # 2) ë¼ë²¨ -> ì¸ë±ìŠ¤ ë§¤í•‘\n",
        "        # Motion-Senseì˜ 6ê°œ í´ë˜ìŠ¤: dws, ups, wlk, jog, sit, std\n",
        "        activities = ['dws', 'jog', 'sit', 'std', 'ups', 'wlk']\n",
        "        self.label2idx = {label: i for i, label in enumerate(activities)}\n",
        "        self.idx2label = {i: label for label, i in self.label2idx.items()}\n",
        "        df_all[\"label_idx\"] = df_all[\"activity\"].map(self.label2idx)\n",
        "\n",
        "        # 3) ì •ê·œí™” (StandardScaler)\n",
        "        # MotionSense ì»¬ëŸ¼: userAcceleration.x/y/z (acc), rotationRate.x/y/z (gyro)\n",
        "        feat_cols = [\n",
        "            \"userAcceleration.x\", \"userAcceleration.y\", \"userAcceleration.z\",\n",
        "            \"rotationRate.x\", \"rotationRate.y\", \"rotationRate.z\"\n",
        "        ]\n",
        "        feats = df_all[feat_cols].values.astype(np.float32)\n",
        "\n",
        "        if self.normalize:\n",
        "            if scaler is None:\n",
        "                # ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì—†ìœ¼ë©´(Trainìš©) -> ìƒˆë¡œ ë§ì¶¤(fit)\n",
        "                self.scaler = StandardScaler()\n",
        "                feats = self.scaler.fit_transform(feats)\n",
        "            else:\n",
        "                # ìŠ¤ì¼€ì¼ëŸ¬ê°€ ìˆìœ¼ë©´(Testìš©) -> ê¸°ì¡´ ê²ƒ ì‚¬ìš©(transform)\n",
        "                self.scaler = scaler\n",
        "                feats = self.scaler.transform(feats)\n",
        "        else:\n",
        "            self.scaler = None\n",
        "\n",
        "        df_all[feat_cols] = feats\n",
        "\n",
        "        # 4) ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± (Subject, Activity, Trial ë³„ë¡œ ê·¸ë£¹í™”)\n",
        "        X_list = []\n",
        "        y_list = []\n",
        "\n",
        "        # trial_idëŠ” ê° csv íŒŒì¼ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ _load_all_dataì—ì„œ ìƒì„±í•´ì•¼ í•¨\n",
        "        for _, g in df_all.groupby([\"subject_id\", \"activity\", \"trial_id\"]):\n",
        "            g = g.sort_values(\"timestamp_idx\").reset_index(drop=True)\n",
        "\n",
        "            data = g[feat_cols].values\n",
        "            labels = g[\"label_idx\"].values\n",
        "            n = len(g)\n",
        "\n",
        "            if n < window_size:\n",
        "                continue\n",
        "\n",
        "            for start in range(0, n - window_size + 1, step_size):\n",
        "                end = start + window_size\n",
        "                w_data = data[start:end]\n",
        "                w_labels = labels[start:end]\n",
        "\n",
        "                # ìœˆë„ìš° ë¼ë²¨ (Mode)\n",
        "                majority_label = np.bincount(w_labels).argmax()\n",
        "\n",
        "                X_list.append(w_data.astype(np.float32))\n",
        "                y_list.append(majority_label)\n",
        "\n",
        "        self.X = np.stack(X_list) if len(X_list) > 0 else np.zeros((0, window_size, 6), dtype=np.float32)\n",
        "        self.y = np.array(y_list, dtype=np.int64)\n",
        "\n",
        "        print(f\"[MotionSenseDataset] windows: {len(self.X)}, classes: {len(self.label2idx)}\")\n",
        "        print(f\"Classes map: {self.label2idx}\")\n",
        "\n",
        "    def _load_all_data(self):\n",
        "        \"\"\"\n",
        "        A_DeviceMotion_data ë‚´ë¶€ì˜ ëª¨ë“  í´ë”ë¥¼ ìˆœíšŒí•˜ë©° CSV ë¡œë“œ\n",
        "        \"\"\"\n",
        "        all_dfs = []\n",
        "\n",
        "        # data_dir ë‚´ë¶€ì˜ í´ë”ë“¤ (ì˜ˆ: dws_1, jog_9 ...)\n",
        "        if not self.data_dir.exists():\n",
        "             raise FileNotFoundError(f\"Directory not found: {self.data_dir}\")\n",
        "\n",
        "        for folder in os.listdir(self.data_dir):\n",
        "            folder_path = self.data_dir / folder\n",
        "            if not folder_path.is_dir():\n",
        "                continue\n",
        "\n",
        "            # í´ë”ëª… íŒŒì‹± (ì˜ˆ: dws_1 -> activity=dws, subject=1)\n",
        "            parts = folder.split('_')\n",
        "            activity_label = parts[0]\n",
        "            subject_id = parts[1]\n",
        "\n",
        "            # í´ë” ë‚´ csv íŒŒì¼ ì½ê¸° (ë³´í†µ sub_1.csv ê°™ì€ í˜•íƒœ)\n",
        "            for csv_file in os.listdir(folder_path):\n",
        "                if not csv_file.endswith(\".csv\"):\n",
        "                    continue\n",
        "\n",
        "                file_path = folder_path / csv_file\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                # Unnamed: 0 ì»¬ëŸ¼ì´ íƒ€ì„ìŠ¤íƒ¬í”„ ì—­í• (ì¸ë±ìŠ¤)\n",
        "                if \"Unnamed: 0\" in df.columns:\n",
        "                    df = df.rename(columns={\"Unnamed: 0\": \"timestamp_idx\"})\n",
        "                else:\n",
        "                    df[\"timestamp_idx\"] = range(len(df))\n",
        "\n",
        "                df[\"activity\"] = activity_label\n",
        "                df[\"subject_id\"] = int(subject_id)\n",
        "                df[\"trial_id\"] = folder  # í´ë”ëª… ìì²´ë¥¼ trial ì‹ë³„ìë¡œ ì‚¬ìš©\n",
        "\n",
        "                all_dfs.append(df)\n",
        "\n",
        "        return pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. ASF Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "class FlowComputer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, s):\n",
        "        B, T, D = s.shape\n",
        "\n",
        "        flow_raw = s[:, 1:, :] - s[:, :-1, :]\n",
        "        flow_mag = torch.norm(flow_raw, dim=-1, keepdim=True)\n",
        "        flow_dir = flow_raw / (flow_mag + 1e-8)\n",
        "\n",
        "        flow_features = torch.cat(\n",
        "            [flow_raw, flow_mag.expand(-1, -1, D), flow_dir],\n",
        "            dim=-1\n",
        "        )\n",
        "        return flow_features, flow_raw, flow_mag\n",
        "\n",
        "class FlowEncoder(nn.Module):\n",
        "    def __init__(self, flow_dim, hidden_dim=64, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.flow_embed = nn.Linear(flow_dim, hidden_dim)\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.flow_conv1 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.flow_conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=1, padding=0)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "    def forward(self, flow_features):\n",
        "        h = self.flow_embed(flow_features)\n",
        "        h_att, _ = self.attention(h, h, h)\n",
        "\n",
        "        h_att = h_att.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.flow_conv1(h_att)))\n",
        "        h = F.relu(self.bn2(self.flow_conv2(h)))\n",
        "\n",
        "        h_pool = torch.mean(h, dim=-1)\n",
        "        return h_pool\n",
        "\n",
        "class StateTransitionPredictor(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, s_t):\n",
        "        B, Tm1, D = s_t.shape\n",
        "        inp = s_t.reshape(B * Tm1, D)\n",
        "        out = self.net(inp)\n",
        "        return out.reshape(B, Tm1, D)\n",
        "\n",
        "class ASFDCLClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_channels=9,\n",
        "                 latent_dim=64,\n",
        "                 hidden_dim=64,\n",
        "                 num_classes=6,\n",
        "                 num_heads=4,\n",
        "                 projection_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "        self.flow_computer = FlowComputer()\n",
        "        self.flow_encoder = FlowEncoder(latent_dim * 3, hidden_dim, num_heads)\n",
        "        self.state_predictor = StateTransitionPredictor(latent_dim, hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.flow_prototypes = nn.Parameter(\n",
        "            torch.randn(num_classes, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_details=False):\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        s_t = s[:, :-1, :]\n",
        "        s_next = s[:, 1:, :]\n",
        "        s_pred_next = self.state_predictor(s_t)\n",
        "\n",
        "        flow_features, flow_raw, flow_mag = self.flow_computer(s)\n",
        "\n",
        "        h = self.flow_encoder(flow_features)\n",
        "\n",
        "        z = self.projection_head(h)\n",
        "        z = F.normalize(z, dim=-1)\n",
        "\n",
        "        logits = self.classifier(h)\n",
        "\n",
        "        if not return_details:\n",
        "            return logits\n",
        "\n",
        "        details = {\n",
        "            \"s\": s,\n",
        "            \"s_t\": s_t,\n",
        "            \"s_next\": s_next,\n",
        "            \"s_pred_next\": s_pred_next,\n",
        "            \"flow_features\": flow_features,\n",
        "            \"flow_raw\": flow_raw,\n",
        "            \"flow_mag\": flow_mag,\n",
        "            \"h\": h,\n",
        "            \"z\": z,\n",
        "            \"prototypes\": self.flow_prototypes\n",
        "        }\n",
        "        return logits, details\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Dynamics-aware Contrastive Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_contrastive_loss(z, labels, temperature=0.07):\n",
        "    B = z.shape[0]\n",
        "    device = z.device\n",
        "\n",
        "    sim_matrix = torch.mm(z, z.t()) / temperature\n",
        "\n",
        "    labels_expanded = labels.unsqueeze(1)\n",
        "    positive_mask = (labels_expanded == labels_expanded.t()).float()\n",
        "\n",
        "    positive_mask = positive_mask - torch.eye(B, device=device)\n",
        "\n",
        "    mask = torch.eye(B, device=device).bool()\n",
        "    sim_matrix_masked = sim_matrix.masked_fill(mask, float('-inf'))\n",
        "\n",
        "    exp_sim = torch.exp(sim_matrix_masked)\n",
        "\n",
        "    pos_sim = (exp_sim * positive_mask).sum(dim=1)\n",
        "\n",
        "    all_sim = exp_sim.sum(dim=1)\n",
        "\n",
        "    has_positive = positive_mask.sum(dim=1) > 0\n",
        "\n",
        "    if has_positive.sum() == 0:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "\n",
        "    loss = -torch.log(pos_sim[has_positive] / (all_sim[has_positive] + 1e-8))\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. ASF-DCL Losses: CE + L_dyn + L_flow_prior + L_proto + L_contrast\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_asf_dcl_losses(logits, details, labels,\n",
        "                           lambda_dyn=0.1,\n",
        "                           lambda_flow=0.05,\n",
        "                           lambda_proto=0.1,\n",
        "                           lambda_contrast=0.15,\n",
        "                           dyn_classes=(0, 1, 4, 5),\n",
        "                           static_classes=(2, 3),\n",
        "                           dyn_target=0.7,\n",
        "                           static_target=0.1,\n",
        "                           proto_tau=0.1,\n",
        "                           contrast_temp=0.07):\n",
        "    device = logits.device\n",
        "\n",
        "    cls_loss = F.cross_entropy(logits, labels, label_smoothing=0.05)\n",
        "\n",
        "    s_next = details[\"s_next\"]\n",
        "    s_pred_next = details[\"s_pred_next\"]\n",
        "    dyn_loss = F.mse_loss(s_pred_next, s_next)\n",
        "\n",
        "    flow_mag = details[\"flow_mag\"]\n",
        "    B, Tm1, _ = flow_mag.shape\n",
        "    flow_mean = flow_mag.mean(dim=1).view(B)\n",
        "\n",
        "    dyn_mask = torch.zeros_like(flow_mean, dtype=torch.bool)\n",
        "    static_mask = torch.zeros_like(flow_mean, dtype=torch.bool)\n",
        "    for c in dyn_classes:\n",
        "        dyn_mask = dyn_mask | (labels == c)\n",
        "    for c in static_classes:\n",
        "        static_mask = static_mask | (labels == c)\n",
        "\n",
        "    flow_prior_loss = torch.tensor(0.0, device=device)\n",
        "    if dyn_mask.any():\n",
        "        dyn_flow = flow_mean[dyn_mask]\n",
        "        flow_prior_loss = flow_prior_loss + F.mse_loss(\n",
        "            dyn_flow, torch.full_like(dyn_flow, dyn_target)\n",
        "        )\n",
        "    if static_mask.any():\n",
        "        static_flow = flow_mean[static_mask]\n",
        "        flow_prior_loss = flow_prior_loss + F.mse_loss(\n",
        "            static_flow, torch.full_like(static_flow, static_target)\n",
        "        )\n",
        "\n",
        "    h = details[\"h\"]\n",
        "    prototypes = details[\"prototypes\"]\n",
        "\n",
        "    h_norm = F.normalize(h, dim=-1)\n",
        "    proto_norm = F.normalize(prototypes, dim=-1)\n",
        "\n",
        "    sim = h_norm @ proto_norm.t()\n",
        "    proto_logits = sim / proto_tau\n",
        "    proto_loss = F.cross_entropy(proto_logits, labels, label_smoothing=0.05)\n",
        "\n",
        "    z = details[\"z\"]\n",
        "    contrast_loss = compute_contrastive_loss(z, labels, temperature=contrast_temp)\n",
        "\n",
        "    total_loss = (\n",
        "        cls_loss +\n",
        "        lambda_dyn * dyn_loss +\n",
        "        lambda_flow * flow_prior_loss +\n",
        "        lambda_proto * proto_loss +\n",
        "        lambda_contrast * contrast_loss\n",
        "    )\n",
        "\n",
        "    loss_dict = {\n",
        "        \"total\": total_loss.item(),\n",
        "        \"cls\": cls_loss.item(),\n",
        "        \"dyn\": dyn_loss.item(),\n",
        "        \"flow_prior\": flow_prior_loss.item(),\n",
        "        \"proto\": proto_loss.item(),\n",
        "        \"contrast\": contrast_loss.item()\n",
        "    }\n",
        "    return total_loss, loss_dict\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Train / Evaluation\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device,\n",
        "                lambda_dyn=0.1, lambda_flow=0.05,\n",
        "                lambda_proto=0.1, lambda_contrast=0.15):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    loss_accumulator = {\n",
        "        \"cls\": 0.0,\n",
        "        \"dyn\": 0.0,\n",
        "        \"flow_prior\": 0.0,\n",
        "        \"proto\": 0.0,\n",
        "        \"contrast\": 0.0\n",
        "    }\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits, details = model(x, return_details=True)\n",
        "        loss, loss_dict = compute_asf_dcl_losses(\n",
        "            logits, details, y,\n",
        "            lambda_dyn=lambda_dyn,\n",
        "            lambda_flow=lambda_flow,\n",
        "            lambda_proto=lambda_proto,\n",
        "            lambda_contrast=lambda_contrast\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        for k in loss_accumulator.keys():\n",
        "            loss_accumulator[k] += loss_dict[k]\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    for k in loss_accumulator.keys():\n",
        "        loss_accumulator[k] /= len(dataloader)\n",
        "\n",
        "    return avg_loss, acc, f1, loss_accumulator\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, acc, f1, cm\n",
        "\n",
        "def evaluate_with_bias(model, dataloader, device, bias):\n",
        "    \"\"\"\n",
        "    Sensor Drift (Bias) Experiment Function\n",
        "    bias: The constant value 'c' to add to the input signal (x + c)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # --- [Modification for Sensor Drift] ---\n",
        "            if bias != 0:\n",
        "                # Add constant 'c' to the original signal x\n",
        "                x = x + bias\n",
        "            # ---------------------------------------\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro') # Using weighted as per your code\n",
        "\n",
        "    return acc, f1\n",
        "\n",
        "def analyze_mechanism(model, dataloader, device, bias, static_indices=[2, 3]):\n",
        "    \"\"\"\n",
        "    ì‹¤í—˜ 1 & 2ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” ë¶„ì„ í•¨ìˆ˜\n",
        "    - ì‹¤í—˜ 1: Bias ìœ ë¬´ì— ë”°ë¥¸ ì •ì  í´ë˜ìŠ¤ì˜ Flow Magnitude í‰ê·  ë³€í™” ì¸¡ì •\n",
        "    - ì‹¤í—˜ 2: Bias ìœ ë¬´ì— ë”°ë¥¸ Confusion Matrix ë³€í™” í™•ì¸\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Flow Magnitudeë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "    static_flows = []\n",
        "    dynamic_flows = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # [Drift Injection]\n",
        "            if bias != 0:\n",
        "                x = x + bias\n",
        "\n",
        "            # ëª¨ë¸ ì¶”ë¡  (Details ë°˜í™˜ í™œì„±í™”)\n",
        "            logits, details = model(x, return_details=True)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "            # [Flow Magnitude ì¶”ì¶œ]\n",
        "            # flow_mag shape: [Batch, Time-1, 1] -> ë°°ì¹˜ì˜ ê° ìƒ˜í”Œë³„ í‰ê·  Flow ê³„ì‚°\n",
        "            # [Batch]\n",
        "            avg_flow_per_sample = details['flow_mag'].mean(dim=1).mean(dim=1).cpu().numpy()\n",
        "\n",
        "            # ì •ì  í´ë˜ìŠ¤ì™€ ë™ì  í´ë˜ìŠ¤ë¡œ ë‚˜ëˆ„ì–´ Flow ì €ì¥\n",
        "            y_np = y.cpu().numpy()\n",
        "            for i in range(len(y_np)):\n",
        "                label = y_np[i]\n",
        "                flow_val = avg_flow_per_sample[i]\n",
        "\n",
        "                if label in static_indices:\n",
        "                    static_flows.append(flow_val)\n",
        "                else:\n",
        "                    dynamic_flows.append(flow_val)\n",
        "\n",
        "    # ê²°ê³¼ ê³„ì‚°\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    avg_static_flow = np.mean(static_flows) if len(static_flows) > 0 else 0.0\n",
        "    avg_dynamic_flow = np.mean(dynamic_flows) if len(dynamic_flows) > 0 else 0.0\n",
        "\n",
        "    return acc, cm, avg_static_flow, avg_dynamic_flow\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Main Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data/motion-sense'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    LAMBDA_DYN = 0.05\n",
        "    LAMBDA_FLOW = 0.02\n",
        "    LAMBDA_PROTO = 0.05\n",
        "    LAMBDA_CONTRAST = 0.2\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(\"ASF-DCL: Action State Flow with Dynamics-aware Contrastive Learning\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Batch size: {BATCH_SIZE}\")\n",
        "    print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "    print()\n",
        "    print(f\"Lambda_dyn: {LAMBDA_DYN}\")\n",
        "    print(f\"Lambda_flow:  {LAMBDA_FLOW}\")\n",
        "    print(f\"Lambda_proto: {LAMBDA_PROTO}\")\n",
        "    print(f\"Lambda_contrast: {LAMBDA_CONTRAST}\")\n",
        "    print()\n",
        "\n",
        "    full_dataset = MotionSenseDataset(DATA_PATH, window_size=128, step_size=64, target_subjects=None, normalize=False)\n",
        "\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "\n",
        "    g_split = torch.Generator().manual_seed(SEED)\n",
        "    train_subset, test_subset = random_split(full_dataset, [train_size, test_size], generator=g_split)\n",
        "    print(f\"Train size: {len(train_subset)}, Test size: {len(test_subset)}\")\n",
        "\n",
        "    train_indices = train_subset.indices\n",
        "    train_data = full_dataset.X[train_indices]\n",
        "\n",
        "    N, T, C = train_data.shape\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_data.reshape(-1, C))\n",
        "\n",
        "    full_N, full_T, full_C = full_dataset.X.shape\n",
        "    full_scaled = scaler.transform(full_dataset.X.reshape(-1, full_C))\n",
        "    full_dataset.X = full_scaled.reshape(full_N, full_T, full_C)\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, num_workers=2,\n",
        "                              worker_init_fn=seed_worker,\n",
        "                              generator=g)\n",
        "    test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, num_workers=2,\n",
        "                             worker_init_fn=seed_worker,\n",
        "                             generator=g)\n",
        "\n",
        "    model = ASFDCLClassifier(\n",
        "        input_channels=6,\n",
        "        latent_dim=64,\n",
        "        hidden_dim=64,\n",
        "        num_classes=6,\n",
        "        num_heads=4,\n",
        "        projection_dim=128\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print()\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=LEARNING_RATE,\n",
        "                                 weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"TRAINING\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, train_f1, loss_dict = train_epoch(\n",
        "            model, train_loader, optimizer, DEVICE,\n",
        "            lambda_dyn=LAMBDA_DYN,\n",
        "            lambda_flow=LAMBDA_FLOW,\n",
        "            lambda_proto=LAMBDA_PROTO,\n",
        "            lambda_contrast=LAMBDA_CONTRAST\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc, test_f1, test_cm = evaluate(\n",
        "            model, test_loader, DEVICE\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_acc = test_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        best_acc = max(best_acc, test_acc)\n",
        "        best_f1 = max(best_f1, test_f1)\n",
        "\n",
        "        log_msg = (f\"[{epoch+1:02d}/{NUM_EPOCHS}] \"\n",
        "                   f\"Train Loss: {train_loss:.3f} | F1: {train_f1:.4f}  |  \"\n",
        "                   f\"Test F1: {test_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "        print(log_msg)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"EVALUATION...\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    test_loss, test_acc, test_f1, test_cm = evaluate(\n",
        "        model, test_loader, DEVICE\n",
        "    )\n",
        "    print(f\"Final Result â†’ Best Test F1: {best_f1:.4f} (Acc: {best_acc:.4f})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" ğŸ§ª HYPOTHESIS VERIFICATION EXPERIMENTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    # Motion-Sense Class Indices\n",
        "    # 'dws':0, 'jog':1, 'sit':2, 'std':3, 'ups':4, 'wlk':5\n",
        "    # Static Classes: sit(2), std(3)\n",
        "    static_idxs = [2, 3]\n",
        "    activities = ['dws', 'jog', 'sit', 'std', 'ups', 'wlk']\n",
        "\n",
        "    # 1. Baseline Analysis (Bias = 0.0)\n",
        "    acc_0, cm_0, static_flow_0, dynamic_flow_0 = analyze_mechanism(\n",
        "        model, test_loader, DEVICE, bias=0.0, static_indices=static_idxs\n",
        "    )\n",
        "\n",
        "    # 2. Drift Analysis (Bias = 0.1) -> ë¬¸ì œê°€ ë°œìƒí–ˆë˜ êµ¬ê°„\n",
        "    acc_bias, cm_bias, static_flow_bias, dynamic_flow_bias = analyze_mechanism(\n",
        "        model, test_loader, DEVICE, bias=0.1, static_indices=static_idxs\n",
        "    )\n",
        "\n",
        "    print(f\"\\n[Experiment 1] Flow Magnitude Analysis (Sensitivity Check)\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Condition':<15} | {'Avg Flow (Static)':<20} | {'Avg Flow (Dynamic)':<20}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Bias 0.0':<15} | {static_flow_0:.6f}{'':<12} | {dynamic_flow_0:.6f}\")\n",
        "    print(f\"{'Bias 0.1':<15} | {static_flow_bias:.6f} (+{static_flow_bias-static_flow_0:.4f})  | {dynamic_flow_bias:.6f}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(\">> ì¦ëª… í¬ì¸íŠ¸: Bias 0.1ì¼ ë•Œ Static Flow ê°’ì´ ê¸‰ê²©íˆ ìƒìŠ¹í–ˆëŠ”ê°€? (YESì—¬ì•¼ í•¨)\")\n",
        "\n",
        "    print(f\"\\n[Experiment 2] Confusion Matrix Pattern (Misclassification Check)\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # CM ì¶œë ¥ì„ ë³´ê¸° ì¢‹ê²Œ DataFrameìœ¼ë¡œ ë³€í™˜\n",
        "    cm_df_0 = pd.DataFrame(cm_0, index=activities, columns=activities)\n",
        "    cm_df_bias = pd.DataFrame(cm_bias, index=activities, columns=activities)\n",
        "\n",
        "    print(\"\\n< Confusion Matrix (Bias = 0.0) >\")\n",
        "    print(cm_df_0)\n",
        "    print(f\"\\nAccuracy: {acc_0:.4f}\")\n",
        "\n",
        "    print(\"\\n< Confusion Matrix (Bias = 0.1) >\")\n",
        "    print(cm_df_bias)\n",
        "    print(f\"\\nAccuracy: {acc_bias:.4f}\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(\">> ì¦ëª… í¬ì¸íŠ¸: Bias 0.1ì—ì„œ 'sit', 'std' í–‰(Row)ì´ 'wlk', 'jog' ì—´(Column)ë¡œ ì´ë™í–ˆëŠ”ê°€?\")\n",
        "    print(\">> ì¦‰, Static -> Dynamic ì˜¤ë¶„ë¥˜ê°€ ë°œìƒí–ˆëŠ”ê°€?\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}