{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwplK6iZguSL",
        "outputId": "1684ed17-f2f9-439b-b700-9b6fd9654323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Standard 1D-CNN Baseline on cuda\n",
            "\n",
            "Starting Training (Standard CNN)...\n",
            "Epoch [10/50] Train F1: 0.9563 | Test F1: 0.9291 (Best: 0.9393)\n",
            "Epoch [20/50] Train F1: 0.9620 | Test F1: 0.9267 (Best: 0.9488)\n",
            "Epoch [30/50] Train F1: 0.9633 | Test F1: 0.9491 (Best: 0.9544)\n",
            "Epoch [40/50] Train F1: 0.9655 | Test F1: 0.9367 (Best: 0.9544)\n",
            "Epoch [50/50] Train F1: 0.9681 | Test F1: 0.9335 (Best: 0.9544)\n",
            "\n",
            "============================================================\n",
            " EXPERIMENT: Baseline Noise Robustness (Best F1: 0.9544)\n",
            "============================================================\n",
            "Sigma      | Accuracy   | F1-Score  \n",
            "------------------------------------\n",
            "0.0        | 0.9532     | 0.9544    \n",
            "0.1        | 0.9528     | 0.9535    \n",
            "0.2        | 0.7727     | 0.7483    \n",
            "0.3        | 0.6861     | 0.6445    \n",
            "0.4        | 0.6447     | 0.5979    \n",
            "0.5        | 0.6006     | 0.5537    \n",
            "------------------------------------\n",
            "Baseline Experiment Completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 0. Utils & Setup\n",
        "# ------------------------------------------------------------------------------\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. Dataset (UCI-HAR) - 기존과 동일\n",
        "# ------------------------------------------------------------------------------\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_path, split='train'):\n",
        "        self.split = split\n",
        "        if split == 'train':\n",
        "            y = np.loadtxt(os.path.join(data_path, 'train', 'y_train.txt'))\n",
        "            signal_path = os.path.join(data_path, 'train', 'Inertial Signals')\n",
        "        else:\n",
        "            y = np.loadtxt(os.path.join(data_path, 'test', 'y_test.txt'))\n",
        "            signal_path = os.path.join(data_path, 'test', 'Inertial Signals')\n",
        "\n",
        "        signals = []\n",
        "        signal_files = [\n",
        "            'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
        "            'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
        "            'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
        "        ]\n",
        "\n",
        "        for signal_file in signal_files:\n",
        "            filename = os.path.join(signal_path, f'{signal_file}_{split}.txt')\n",
        "            signal_data = np.loadtxt(filename)\n",
        "            signals.append(signal_data)\n",
        "\n",
        "        self.X = np.stack(signals, axis=-1).astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.X[idx]), torch.LongTensor([self.y[idx]])[0]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Baseline Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "# ASF-DCL과 공정한 비교를 위해 동일한 Encoder 구조 사용\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "# Baseline Model: Encoder + Global Average Pooling + Classifier\n",
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64, num_classes=6, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "\n",
        "        # Flow 모듈 없이 바로 분류 (일반적인 CNN 구조)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Encode: [Batch, Time, Dim]\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        # 2. Global Average Pooling (Time 축 평균)\n",
        "        s_pool = torch.mean(s, dim=1)\n",
        "\n",
        "        # 3. Classify\n",
        "        logits = self.classifier(s_pool)\n",
        "        return logits\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Train & Evaluate Functions (Baseline용)\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=0.05)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, f1\n",
        "\n",
        "def evaluate_with_noise(model, dataloader, device, sigma):\n",
        "    \"\"\"\n",
        "    AWGN 노이즈를 주입하여 모델의 견고성을 평가하는 함수\n",
        "    sigma: 노이즈 강도 (Standard Deviation)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # --- Noise Injection ---\n",
        "            if sigma > 0:\n",
        "                noise = torch.randn_like(x) * sigma\n",
        "                x = x + noise\n",
        "            # -----------------------\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    # 설정 (기존과 동일하게 맞춤)\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data/UCI_HAR' # 경로 확인 필요\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Running Standard 1D-CNN Baseline on {DEVICE}\")\n",
        "\n",
        "    # 데이터 로드\n",
        "    train_dataset = UCIHARDataset(DATA_PATH, split='train')\n",
        "    test_dataset = UCIHARDataset(DATA_PATH, split='test')\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              worker_init_fn=seed_worker, generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = StandardCNN(input_channels=9, latent_dim=64, num_classes=6).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    print(\"\\nStarting Training (Standard CNN)...\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t_loss, t_f1 = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "\n",
        "        # Validation (Noise=0.0)\n",
        "        v_acc, v_f1 = evaluate_with_noise(model, test_loader, DEVICE, sigma=0.0)\n",
        "\n",
        "        if v_f1 > best_f1:\n",
        "            best_f1 = v_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train F1: {t_f1:.4f} | Test F1: {v_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "\n",
        "    # --- 실험: AWGN Robustness ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\" EXPERIMENT: Baseline Noise Robustness (Best F1: {best_f1:.4f})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 최고 성능 모델 로드\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    sigma_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "    print(f\"{'Sigma':<10} | {'Accuracy':<10} | {'F1-Score':<10}\")\n",
        "    print(\"-\" * 36)\n",
        "\n",
        "    for sigma in sigma_levels:\n",
        "        acc, f1 = evaluate_with_noise(model, test_loader, DEVICE, sigma=sigma)\n",
        "        print(f\"{sigma:<10.1f} | {acc:<10.4f} | {f1:<10.4f}\")\n",
        "\n",
        "    print(\"-\" * 36)\n",
        "    print(\"Baseline Experiment Completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}