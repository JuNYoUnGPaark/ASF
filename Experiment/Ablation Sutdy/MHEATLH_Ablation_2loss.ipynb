{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn.manifold import TSNE\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. MHEALTH Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "def _load_single_mhealth_log(path: str, feature_cols: list[str]):\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        names=feature_cols + [\"label\"],\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def load_mhealth_dataframe(data_dir: str):\n",
        "    feature_cols = [\n",
        "        \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",      # 0-2\n",
        "        \"ecg_1\", \"ecg_2\",                                 # 3-4\n",
        "        \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",      # 5-7\n",
        "        \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",   # 8-10\n",
        "        \"mag_ankle_x\", \"mag_ankle_y\", \"mag_ankle_z\",      # 11-13\n",
        "        \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",            # 14-16\n",
        "        \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",         # 17-19\n",
        "        \"mag_arm_x\", \"mag_arm_y\", \"mag_arm_z\",            # 20-22\n",
        "    ]\n",
        "\n",
        "    log_files = glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\"))\n",
        "    if not log_files:\n",
        "         raise FileNotFoundError(f\"No mHealth_subject*.log files found in {data_dir}\")\n",
        "    print(f\"Found {len(log_files)} log files in {data_dir}\")\n",
        "\n",
        "    dfs = []\n",
        "    for fp in log_files:\n",
        "        df_i = _load_single_mhealth_log(fp, feature_cols)\n",
        "        dfs.append(df_i)\n",
        "\n",
        "    full_df = pd.concat(dfs, ignore_index=True)\n",
        "    full_df = full_df[full_df[\"label\"] != 0].copy()\n",
        "    full_df.loc[:, \"label\"] = full_df[\"label\"] - 1\n",
        "\n",
        "    return full_df, feature_cols\n",
        "\n",
        "def create_mhealth_windows(df: pd.DataFrame, feature_cols: list[str], window_size: int, step_size: int):\n",
        "    data_arr = df[feature_cols].to_numpy(dtype=np.float32)\n",
        "    labels_arr = df[\"label\"].to_numpy(dtype=np.int64)\n",
        "    L = data_arr.shape[0]\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    start = 0\n",
        "    while start + window_size <= L:\n",
        "        end = start + window_size\n",
        "        window_x = data_arr[start:end]\n",
        "        window_label = labels_arr[end - 1]\n",
        "        window_x_ct = np.transpose(window_x, (1, 0))\n",
        "\n",
        "        X_list.append(window_x_ct)\n",
        "        y_list.append(int(window_label))\n",
        "        start += step_size\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No windows created.\")\n",
        "\n",
        "    X_np = np.stack(X_list, axis=0).astype(np.float32)\n",
        "    y_np = np.array(y_list, dtype=np.int64)\n",
        "    return X_np, y_np\n",
        "\n",
        "class MHEALTHDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, window_size: int = 128, step_size: int = 64):\n",
        "        super().__init__()\n",
        "        full_df, feature_cols = load_mhealth_dataframe(data_dir)\n",
        "        X, y = create_mhealth_windows(full_df, feature_cols, window_size, step_size)\n",
        "\n",
        "        self.X = np.transpose(X, (0, 2, 1)).astype(np.float32)\n",
        "        self.y = y\n",
        "\n",
        "        self.label_names = [\n",
        "            \"Standing still\", \"Sitting\", \"Lying down\",   # 0, 1, 2 (Static)\n",
        "            \"Walking\", \"Climbing stairs\", \"Waist bends\", # 3, 4, 5\n",
        "            \"Arms elevation\", \"Knees bending\", \"Cycling\",# 6, 7, 8\n",
        "            \"Jogging\", \"Running\", \"Jump front&back\",     # 9, 10, 11\n",
        "        ]\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Loaded MHEALTH dataset\")\n",
        "        print(f\"  X shape : {self.X.shape}  (N, T, C)\")\n",
        "        print(f\"  y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"  Classes : {len(self.label_names)}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # subject 정보가 필요하다면 3번째 반환값으로 넣고 collate_fn을 수정해야 함.\n",
        "        return torch.FloatTensor(self.X[idx]), torch.LongTensor([self.y[idx]])[0]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. ASF Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "class FlowComputer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, s):\n",
        "        B, T, D = s.shape\n",
        "\n",
        "        flow_raw = s[:, 1:, :] - s[:, :-1, :]\n",
        "        flow_mag = torch.norm(flow_raw, dim=-1, keepdim=True)\n",
        "        flow_dir = flow_raw / (flow_mag + 1e-8)\n",
        "\n",
        "        flow_features = torch.cat(\n",
        "            [flow_raw, flow_mag.expand(-1, -1, D), flow_dir],\n",
        "            dim=-1\n",
        "        )\n",
        "        return flow_features, flow_raw, flow_mag\n",
        "\n",
        "class FlowEncoder(nn.Module):\n",
        "    def __init__(self, flow_dim, hidden_dim=64, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.flow_embed = nn.Linear(flow_dim, hidden_dim)\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.flow_conv1 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.flow_conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=1, padding=0)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "    def forward(self, flow_features):\n",
        "        h = self.flow_embed(flow_features)\n",
        "        h_att, _ = self.attention(h, h, h)\n",
        "\n",
        "        h_att = h_att.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.flow_conv1(h_att)))\n",
        "        h = F.relu(self.bn2(self.flow_conv2(h)))\n",
        "\n",
        "        h_pool = torch.mean(h, dim=-1)\n",
        "        return h_pool\n",
        "\n",
        "class StateTransitionPredictor(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, s_t):\n",
        "        B, Tm1, D = s_t.shape\n",
        "        inp = s_t.reshape(B * Tm1, D)\n",
        "        out = self.net(inp)\n",
        "        return out.reshape(B, Tm1, D)\n",
        "\n",
        "class ASFDCLClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_channels=9,\n",
        "                 latent_dim=64,\n",
        "                 hidden_dim=64,\n",
        "                 num_classes=6,\n",
        "                 num_heads=4,\n",
        "                 projection_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "        self.flow_computer = FlowComputer()\n",
        "        self.flow_encoder = FlowEncoder(latent_dim * 3, hidden_dim, num_heads)\n",
        "        self.state_predictor = StateTransitionPredictor(latent_dim, hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.flow_prototypes = nn.Parameter(\n",
        "            torch.randn(num_classes, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_details=False):\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        s_t = s[:, :-1, :]\n",
        "        s_next = s[:, 1:, :]\n",
        "        s_pred_next = self.state_predictor(s_t)\n",
        "\n",
        "        flow_features, flow_raw, flow_mag = self.flow_computer(s)\n",
        "\n",
        "        h = self.flow_encoder(flow_features)\n",
        "\n",
        "        z = self.projection_head(h)\n",
        "        z = F.normalize(z, dim=-1)\n",
        "\n",
        "        logits = self.classifier(h)\n",
        "\n",
        "        if not return_details:\n",
        "            return logits\n",
        "\n",
        "        details = {\n",
        "            \"s\": s,\n",
        "            \"s_t\": s_t,\n",
        "            \"s_next\": s_next,\n",
        "            \"s_pred_next\": s_pred_next,\n",
        "            \"flow_features\": flow_features,\n",
        "            \"flow_raw\": flow_raw,\n",
        "            \"flow_mag\": flow_mag,\n",
        "            \"h\": h,\n",
        "            \"z\": z,\n",
        "            \"prototypes\": self.flow_prototypes\n",
        "        }\n",
        "        return logits, details\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Dynamics-aware Contrastive Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_contrastive_loss(z, labels, temperature=0.07):\n",
        "    B = z.shape[0]\n",
        "    device = z.device\n",
        "\n",
        "    sim_matrix = torch.mm(z, z.t()) / temperature\n",
        "\n",
        "    labels_expanded = labels.unsqueeze(1)\n",
        "    positive_mask = (labels_expanded == labels_expanded.t()).float()\n",
        "\n",
        "    positive_mask = positive_mask - torch.eye(B, device=device)\n",
        "\n",
        "    mask = torch.eye(B, device=device).bool()\n",
        "    sim_matrix_masked = sim_matrix.masked_fill(mask, float('-inf'))\n",
        "\n",
        "    exp_sim = torch.exp(sim_matrix_masked)\n",
        "\n",
        "    pos_sim = (exp_sim * positive_mask).sum(dim=1)\n",
        "\n",
        "    all_sim = exp_sim.sum(dim=1)\n",
        "\n",
        "    has_positive = positive_mask.sum(dim=1) > 0\n",
        "\n",
        "    if has_positive.sum() == 0:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "\n",
        "    loss = -torch.log(pos_sim[has_positive] / (all_sim[has_positive] + 1e-8))\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. ASF-DCL Losses: CE + L_dyn + L_flow_prior + L_proto + L_contrast\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_asf_dcl_losses(logits, details, labels,\n",
        "                           lambda_dyn=0.1,\n",
        "                           lambda_flow=0.05,\n",
        "                           lambda_proto=0.1,\n",
        "                           lambda_contrast=0.15,\n",
        "                           dyn_classes=(0, 1, 2),\n",
        "                           static_classes=(3, 4, 5),\n",
        "                           dyn_target=0.7,\n",
        "                           static_target=0.1,\n",
        "                           proto_tau=0.1,\n",
        "                           contrast_temp=0.07):\n",
        "    device = logits.device\n",
        "\n",
        "    cls_loss = F.cross_entropy(logits, labels, label_smoothing=0.05)\n",
        "\n",
        "    s_next = details[\"s_next\"]\n",
        "    s_pred_next = details[\"s_pred_next\"]\n",
        "    dyn_loss = F.mse_loss(s_pred_next, s_next)\n",
        "\n",
        "    flow_mag = details[\"flow_mag\"]\n",
        "    B, Tm1, _ = flow_mag.shape\n",
        "    flow_mean = flow_mag.mean(dim=1).view(B)\n",
        "\n",
        "    dyn_mask = torch.zeros_like(flow_mean, dtype=torch.bool)\n",
        "    static_mask = torch.zeros_like(flow_mean, dtype=torch.bool)\n",
        "    for c in dyn_classes:\n",
        "        dyn_mask = dyn_mask | (labels == c)\n",
        "    for c in static_classes:\n",
        "        static_mask = static_mask | (labels == c)\n",
        "\n",
        "    flow_prior_loss = torch.tensor(0.0, device=device)\n",
        "    if dyn_mask.any():\n",
        "        dyn_flow = flow_mean[dyn_mask]\n",
        "        flow_prior_loss = flow_prior_loss + F.mse_loss(\n",
        "            dyn_flow, torch.full_like(dyn_flow, dyn_target)\n",
        "        )\n",
        "    if static_mask.any():\n",
        "        static_flow = flow_mean[static_mask]\n",
        "        flow_prior_loss = flow_prior_loss + F.mse_loss(\n",
        "            static_flow, torch.full_like(static_flow, static_target)\n",
        "        )\n",
        "\n",
        "    h = details[\"h\"]\n",
        "    prototypes = details[\"prototypes\"]\n",
        "\n",
        "    h_norm = F.normalize(h, dim=-1)\n",
        "    proto_norm = F.normalize(prototypes, dim=-1)\n",
        "\n",
        "    sim = h_norm @ proto_norm.t()\n",
        "    proto_logits = sim / proto_tau\n",
        "    proto_loss = F.cross_entropy(proto_logits, labels, label_smoothing=0.05)\n",
        "\n",
        "    z = details[\"z\"]\n",
        "    contrast_loss = compute_contrastive_loss(z, labels, temperature=contrast_temp)\n",
        "\n",
        "    total_loss = (\n",
        "        cls_loss +\n",
        "        lambda_dyn * dyn_loss +\n",
        "        lambda_flow * flow_prior_loss +\n",
        "        lambda_proto * proto_loss +\n",
        "        lambda_contrast * contrast_loss\n",
        "    )\n",
        "\n",
        "    loss_dict = {\n",
        "        \"total\": total_loss.item(),\n",
        "        \"cls\": cls_loss.item(),\n",
        "        \"dyn\": dyn_loss.item(),\n",
        "        \"flow_prior\": flow_prior_loss.item(),\n",
        "        \"proto\": proto_loss.item(),\n",
        "        \"contrast\": contrast_loss.item()\n",
        "    }\n",
        "    return total_loss, loss_dict\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Train / Evaluation\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device,\n",
        "                lambda_dyn=0.1, lambda_flow=0.05,\n",
        "                lambda_proto=0.1, lambda_contrast=0.15, **kwargs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    loss_accumulator = {\n",
        "        \"cls\": 0.0,\n",
        "        \"dyn\": 0.0,\n",
        "        \"flow_prior\": 0.0,\n",
        "        \"proto\": 0.0,\n",
        "        \"contrast\": 0.0\n",
        "    }\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits, details = model(x, return_details=True)\n",
        "        loss, loss_dict = compute_asf_dcl_losses(\n",
        "            logits, details, y,\n",
        "            lambda_dyn=lambda_dyn,\n",
        "            lambda_flow=lambda_flow,\n",
        "            lambda_proto=lambda_proto,\n",
        "            lambda_contrast=lambda_contrast,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        for k in loss_accumulator.keys():\n",
        "            loss_accumulator[k] += loss_dict[k]\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    for k in loss_accumulator.keys():\n",
        "        loss_accumulator[k] /= len(dataloader)\n",
        "\n",
        "    return avg_loss, acc, f1, loss_accumulator\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, acc, f1, cm\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Main Training Loop & Ablation Study\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def get_dataloaders(train_dataset, test_dataset, batch_size, seed):\n",
        "    \"\"\"\n",
        "    매번 동일한 순서를 보장하기 위해 Generator와 DataLoader를 새로 생성하는 함수\n",
        "    \"\"\"\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(seed)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "    # Test는 shuffle=False이므로 사실 순서가 고정되지만, 일관성을 위해 같이 생성\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                             num_workers=2, worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def run_training(model_name, train_dataset, test_dataset, batch_size, seed, device,\n",
        "                 lambda_dyn, lambda_flow, lambda_proto, lambda_contrast,\n",
        "                 num_epochs=50, learning_rate=0.001, input_channels=23, num_classes=12):\n",
        "\n",
        "    # [중요] 학습 시작 직전에 시드를 재설정하여 난수 상태 초기화\n",
        "    set_seed(seed)\n",
        "\n",
        "    # [중요] 초기화된 시드로 DataLoader를 새로 생성 -> 항상 동일한 배치 순서 보장\n",
        "    train_loader, test_loader = get_dataloaders(train_dataset, test_dataset, batch_size, seed)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\" START TRAINING: {model_name}\")\n",
        "    print(f\" Configuration -> Lambda_dyn: {lambda_dyn}, Lambda_flow: {lambda_flow}\")\n",
        "    print(f\" Data Order Check -> First batch label sample: {next(iter(train_loader))[1][0].item()}\") # 순서 확인용 로그\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = ASFDCLClassifier(\n",
        "        input_channels=input_channels,\n",
        "        latent_dim=64,\n",
        "        hidden_dim=64,\n",
        "        num_classes=num_classes,\n",
        "        num_heads=4,\n",
        "        projection_dim=128\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_f1 = 0.0\n",
        "    save_path = f'best_model_{model_name}.pth'\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc, train_f1, loss_dict = train_epoch(\n",
        "            model, train_loader, optimizer, device,\n",
        "            lambda_dyn=lambda_dyn,\n",
        "            lambda_flow=lambda_flow,\n",
        "            lambda_proto=lambda_proto,\n",
        "            lambda_contrast=lambda_contrast,\n",
        "            dyn_classes=(3, 4, 5, 6, 7, 8, 9, 10, 11),\n",
        "            static_classes=(0, 1, 2)\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc, test_f1, _ = evaluate(model, test_loader, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"[{epoch+1:02d}/{num_epochs}] \"\n",
        "                  f\"Train Loss: {train_loss:.3f} | Train F1: {train_f1:.4f} | \"\n",
        "                  f\"Test F1: {test_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "\n",
        "    # 최종 로드 및 확인\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    _, final_acc, final_f1, _ = evaluate(model, test_loader, device)\n",
        "\n",
        "    print(f\"\\n>> {model_name} Final Best Result -> Acc: {final_acc:.4f}, F1: {final_f1:.4f}\")\n",
        "    return final_acc, final_f1\n",
        "\n",
        "\n",
        "def main():\n",
        "    SEED = 42\n",
        "    # 초기 시드 설정 (데이터셋 Split용)\n",
        "    set_seed(SEED)\n",
        "\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 25\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    LAMBDA_PROTO = 0.05\n",
        "    LAMBDA_CONTRAST = 0.2\n",
        "\n",
        "    # 1. 데이터셋 로드 (공통)\n",
        "    try:\n",
        "        full_dataset = MHEALTHDataset(DATA_PATH, window_size=128, step_size=64)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. 데이터셋 분할 (고정) - 여기서 시드를 사용하여 분할하므로 구성 요소는 항상 동일\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(SEED)\n",
        "    )\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # Ablation Study 실행\n",
        "    # 여기서는 Loader를 넘기지 않고 Dataset과 Seed를 넘김\n",
        "    # ---------------------------------------------------------\n",
        "    results = {}\n",
        "\n",
        "    # Case 1: Baseline\n",
        "    acc_base, f1_base = run_training(\n",
        "        model_name=\"Baseline\",\n",
        "        train_dataset=train_dataset, # Dataset 전달\n",
        "        test_dataset=test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        seed=SEED,                   # Seed 전달 -> 내부에서 재설정\n",
        "        device=DEVICE,\n",
        "        lambda_dyn=0.0,\n",
        "        lambda_flow=0.0,\n",
        "        lambda_proto=LAMBDA_PROTO,\n",
        "        lambda_contrast=LAMBDA_CONTRAST,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE\n",
        "    )\n",
        "    results['Baseline'] = {'Acc': acc_base, 'F1': f1_base}\n",
        "\n",
        "    # Case 2: Full Model\n",
        "    acc_full, f1_full = run_training(\n",
        "        model_name=\"Full_Model\",\n",
        "        train_dataset=train_dataset, # 동일한 Dataset 전달\n",
        "        test_dataset=test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        seed=SEED,                   # 동일한 Seed 전달 -> 동일한 순서 보장\n",
        "        device=DEVICE,\n",
        "        lambda_dyn=0.05,\n",
        "        lambda_flow=0.02,\n",
        "        lambda_proto=LAMBDA_PROTO,\n",
        "        lambda_contrast=LAMBDA_CONTRAST,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE\n",
        "    )\n",
        "    results['Full_Model'] = {'Acc': acc_full, 'F1': f1_full}\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # 결과 비교\n",
        "    # ---------------------------------------------------------\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" ABLATION STUDY RESULTS COMPARISON\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"{'Model Setting':<20} | {'Test Accuracy':<15} | {'Test F1-Score':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    print(f\"{'Baseline (0.0/0.0)':<20} | {results['Baseline']['Acc']:.4f}{' '*10} | {results['Baseline']['F1']:.4f}\")\n",
        "    print(f\"{'Full Model':<20} | {results['Full_Model']['Acc']:.4f}{' '*10} | {results['Full_Model']['F1']:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    diff_f1 = results['Full_Model']['F1'] - results['Baseline']['F1']\n",
        "    print(f\"Improvement (F1): {diff_f1:+.4f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrCx_fn5lg_E",
        "outputId": "7e7528aa-91fc-40a0-928d-e898fa28ce1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10 log files in /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\n",
            "================================================================================\n",
            "Loaded MHEALTH dataset\n",
            "  X shape : (5361, 128, 23)  (N, T, C)\n",
            "  y shape : (5361,)  (N,)\n",
            "  Classes : 12\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            " START TRAINING: Baseline\n",
            " Configuration -> Lambda_dyn: 0.0, Lambda_flow: 0.0\n",
            " Data Order Check -> First batch label sample: 7\n",
            "================================================================================\n",
            "[10/25] Train Loss: 0.593 | Train F1: 0.9432 | Test F1: 0.9646 (Best: 0.9646)\n",
            "[20/25] Train Loss: 0.433 | Train F1: 0.9905 | Test F1: 0.9765 (Best: 0.9786)\n",
            "\n",
            ">> Baseline Final Best Result -> Acc: 0.9804, F1: 0.9786\n",
            "\n",
            "================================================================================\n",
            " START TRAINING: Full_Model\n",
            " Configuration -> Lambda_dyn: 0.05, Lambda_flow: 0.02\n",
            " Data Order Check -> First batch label sample: 7\n",
            "================================================================================\n",
            "[10/25] Train Loss: 0.464 | Train F1: 0.9800 | Test F1: 0.9739 (Best: 0.9767)\n",
            "[20/25] Train Loss: 0.378 | Train F1: 0.9985 | Test F1: 0.9848 (Best: 0.9863)\n",
            "\n",
            ">> Full_Model Final Best Result -> Acc: 0.9897, F1: 0.9875\n",
            "\n",
            "============================================================\n",
            " ABLATION STUDY RESULTS COMPARISON\n",
            "============================================================\n",
            "Model Setting        | Test Accuracy   | Test F1-Score  \n",
            "------------------------------------------------------------\n",
            "Baseline (0.0/0.0)   | 0.9804           | 0.9786\n",
            "Full Model           | 0.9897           | 0.9875\n",
            "------------------------------------------------------------\n",
            "Improvement (F1): +0.0089\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}
