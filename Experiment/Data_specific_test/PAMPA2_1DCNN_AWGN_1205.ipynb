{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 0. Utils & Setup\n",
        "# ------------------------------------------------------------------------------\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. PAMAP2\n",
        "# ------------------------------------------------------------------------------\n",
        "# ========================\n",
        "# PAMAP2 데이터 로드\n",
        "# ========================\n",
        "def create_pamap2_windows(df: pd.DataFrame, window_size: int, step_size: int):\n",
        "    \"\"\"\n",
        "    subject별로 timestamp 순서대로 전체 시계열을 따라가며 슬라이딩 윈도우 생성.\n",
        "    한 윈도우의 라벨은 마지막 프레임의 activityID.\n",
        "    마지막 라벨이 0(Null/기타) 이면 그 윈도우는 버린다.\n",
        "\n",
        "    Returns:\n",
        "        X:          (N, C, T) float32\n",
        "        y:          (N,) int64  (0..11로 리맵된 레이블)\n",
        "        subj_ids:   (N,) int64\n",
        "        label_names:list[str] 길이 12, new_index -> human-readable\n",
        "    \"\"\"\n",
        "\n",
        "    # 사용할 피처들 (orientation*, heartrate, *_Temperature 등은 제외)\n",
        "    feature_cols = [\n",
        "        # hand\n",
        "        \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "        \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "        \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "        \"handMagne1\",\"handMagne2\",\"handMagne3\",\n",
        "        # chest\n",
        "        \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "        \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "        \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "        \"chestMagne1\",\"chestMagne2\",\"chestMagne3\",\n",
        "        # ankle\n",
        "        \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "        \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "        \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "        \"ankleMagne1\",\"ankleMagne2\",\"ankleMagne3\",\n",
        "    ]\n",
        "\n",
        "    # PAMAP2 실제 activityID들 중 우리가 쓰는 12개 클래스만 남김\n",
        "    # 순서 고정: 이 순서가 new class index 0..11이 된다.\n",
        "    ORDERED_IDS = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
        "\n",
        "    # 원본 activityID -> new index(0..11)\n",
        "    old2new = {\n",
        "        1: 0,   # Lying\n",
        "        2: 1,   # Sitting\n",
        "        3: 2,   # Standing\n",
        "        4: 3,   # Walking\n",
        "        5: 4,   # Running\n",
        "        6: 5,   # Cycling\n",
        "        7: 6,   # Nordic walking\n",
        "        12: 7,  # Ascending stairs\n",
        "        13: 8,  # Descending stairs\n",
        "        16: 9,  # Vacuum cleaning\n",
        "        17: 10, # Ironing\n",
        "        24: 11, # Rope jumping\n",
        "    }\n",
        "\n",
        "    # new index -> 사람이 읽는 이름\n",
        "    label_names = [\n",
        "        \"Lying\",              # 0 -> orig 1\n",
        "        \"Sitting\",            # 1 -> orig 2\n",
        "        \"Standing\",           # 2 -> orig 3\n",
        "        \"Walking\",            # 3 -> orig 4\n",
        "        \"Running\",            # 4 -> orig 5\n",
        "        \"Cycling\",            # 5 -> orig 6\n",
        "        \"Nordic walking\",     # 6 -> orig 7\n",
        "        \"Ascending stairs\",   # 7 -> orig 12\n",
        "        \"Descending stairs\",  # 8 -> orig 13\n",
        "        \"Vacuum cleaning\",    # 9 -> orig 16\n",
        "        \"Ironing\",            # 10 -> orig 17\n",
        "        \"Rope jumping\",       # 11 -> orig 24\n",
        "    ]\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    subj_list = []\n",
        "\n",
        "    # subject별로 끊어서 시간 순 정렬 후 슬라이딩 윈도우\n",
        "    for subj_id, g in df.groupby(\"subject_id\"):\n",
        "        # 시간순 정렬\n",
        "        if \"timestamp\" in g.columns:\n",
        "            g = g.sort_values(\"timestamp\")\n",
        "        else:\n",
        "            g = g.sort_index()\n",
        "\n",
        "        data_arr  = g[feature_cols].to_numpy(dtype=np.float32)   # (L, C)\n",
        "        label_arr = g[\"activityID\"].to_numpy(dtype=np.int64)     # (L,)\n",
        "        L = data_arr.shape[0]\n",
        "\n",
        "        start = 0\n",
        "        while start + window_size <= L:\n",
        "            end = start + window_size\n",
        "\n",
        "            last_label_orig = int(label_arr[end - 1])\n",
        "\n",
        "            # 0 = \"other / null\" → 버림\n",
        "            if last_label_orig == 0:\n",
        "                start += step_size\n",
        "                continue\n",
        "\n",
        "            # 우리가 쓰는 12개 클래스에 없는 라벨이면 버림\n",
        "            if last_label_orig not in old2new:\n",
        "                start += step_size\n",
        "                continue\n",
        "\n",
        "            # 윈도우 추출\n",
        "            window_ct = data_arr[start:end].T  # (T, C) -> (C, T)\n",
        "\n",
        "            X_list.append(window_ct)\n",
        "            y_list.append(old2new[last_label_orig])\n",
        "            subj_list.append(int(subj_id))\n",
        "\n",
        "            start += step_size\n",
        "\n",
        "    # numpy 변환\n",
        "    X = np.stack(X_list, axis=0).astype(np.float32)      # (N, C, T)\n",
        "    y = np.asarray(y_list, dtype=np.int64)               # (N,)\n",
        "    subj_ids = np.asarray(subj_list, dtype=np.int64)     # (N,)\n",
        "\n",
        "    return X, y, subj_ids, label_names\n",
        "\n",
        "class PAMAP2Dataset(Dataset):\n",
        "    def __init__(self, data_dir, window_size, step_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) CSV 전부 읽어서 하나의 df로 합치기\n",
        "        csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
        "        if len(csv_files) == 0:\n",
        "            raise RuntimeError(f\"No CSV files found under {data_dir}\")\n",
        "\n",
        "        dfs = []\n",
        "        for fpath in sorted(csv_files):\n",
        "            df_i = pd.read_csv(fpath)\n",
        "\n",
        "            if \"subject_id\" not in df_i.columns:\n",
        "                m = re.findall(r\"\\d+\", os.path.basename(fpath))\n",
        "                subj_guess = int(m[0]) if len(m) > 0 else 0\n",
        "                df_i[\"subject_id\"] = subj_guess\n",
        "\n",
        "            dfs.append(df_i)\n",
        "\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        df = df.dropna(subset=['activityID'])\n",
        "\n",
        "        # 기본 타입 정리\n",
        "        df[\"activityID\"] = df[\"activityID\"].astype(np.int64)\n",
        "        df[\"subject_id\"] = df[\"subject_id\"].astype(np.int64)\n",
        "        if \"timestamp\" in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "        # ===========================\n",
        "        # (1) NaN 처리\n",
        "        # ===========================\n",
        "        feature_cols = [\n",
        "            # hand\n",
        "            \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "            \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "            \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "            \"handMagne1\",\"handMagne2\",\"handMagne3\",\n",
        "            # chest\n",
        "            \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "            \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "            \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "            \"chestMagne1\",\"chestMagne2\",\"chestMagne3\",\n",
        "            # ankle\n",
        "            \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "            \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "            \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "            \"ankleMagne1\",\"ankleMagne2\",\"ankleMagne3\",\n",
        "        ]\n",
        "\n",
        "        # subject별로 결측치 보간 -> ffill/bfill로 마저 메우기\n",
        "        def _fill_subject_group(g):\n",
        "            # 시간 순으로 정렬 (timestamp 있으면 timestamp 기준)\n",
        "            if \"timestamp\" in g.columns:\n",
        "                g = g.sort_values(\"timestamp\")\n",
        "            else:\n",
        "                g = g.sort_index()\n",
        "\n",
        "            # 각 컬럼별로 interpolate + ffill/bfill\n",
        "            g[feature_cols] = (\n",
        "                g[feature_cols]\n",
        "                .interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
        "                .ffill()\n",
        "                .bfill()\n",
        "            )\n",
        "            return g\n",
        "\n",
        "        df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n",
        "\n",
        "        # 이 시점에서 feature_cols 안에 NaN이 남아있으면 안 됨\n",
        "        # 혹시라도 남았으면 0으로 막아버리기 (safety net)\n",
        "        df[feature_cols] = df[feature_cols].fillna(0.0)\n",
        "\n",
        "        # ===========================\n",
        "        # (2) 스케일 표준화\n",
        "        # ===========================\n",
        "        scaler = StandardScaler()\n",
        "        df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "        # ===========================\n",
        "        # (3) 윈도우 생성\n",
        "        # ===========================\n",
        "        X, y, subj_ids, label_names = create_pamap2_windows(\n",
        "            df,\n",
        "            window_size=window_size,\n",
        "            step_size=step_size,\n",
        "        )\n",
        "\n",
        "        self.X = np.transpose(X, (0, 2, 1))\n",
        "        self.y = y          # (N,)\n",
        "        self.subject_ids = subj_ids\n",
        "        self.label_names = label_names\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long),\n",
        "            self.subject_ids[idx]\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Baseline Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "# ASF-DCL과 공정한 비교를 위해 동일한 Encoder 구조 사용\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "# Baseline Model: Encoder + Global Average Pooling + Classifier\n",
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64, num_classes=6, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "\n",
        "        # Flow 모듈 없이 바로 분류 (일반적인 CNN 구조)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Encode: [Batch, Time, Dim]\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        # 2. Global Average Pooling (Time 축 평균)\n",
        "        s_pool = torch.mean(s, dim=1)\n",
        "\n",
        "        # 3. Classify\n",
        "        logits = self.classifier(s_pool)\n",
        "        return logits\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Train & Evaluate Functions (Baseline용)\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=0.05)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, f1\n",
        "\n",
        "def evaluate_with_noise(model, dataloader, device, sigma):\n",
        "    \"\"\"\n",
        "    AWGN 노이즈를 주입하여 모델의 견고성을 평가하는 함수\n",
        "    sigma: 노이즈 강도 (Standard Deviation)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            # --- Noise Injection ---\n",
        "            if sigma > 0:\n",
        "                noise = torch.randn_like(x) * sigma\n",
        "                x = x + noise\n",
        "            # -----------------------\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    # 설정 (기존과 동일하게 맞춤)\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Running Standard 1D-CNN Baseline on {DEVICE}\")\n",
        "\n",
        "    # 데이터 로드\n",
        "    full_dataset = PAMAP2Dataset(DATA_PATH, window_size=128, step_size=64)\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(total_size * 0.8)\n",
        "    test_size = total_size - train_size\n",
        "\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(SEED)\n",
        "    )\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              worker_init_fn=seed_worker, generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = StandardCNN(input_channels=36, latent_dim=64, num_classes=12).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    print(\"\\nStarting Training (Standard CNN)...\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t_loss, t_f1 = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "\n",
        "        # Validation (Noise=0.0)\n",
        "        v_acc, v_f1 = evaluate_with_noise(model, test_loader, DEVICE, sigma=0.0)\n",
        "\n",
        "        if v_f1 > best_f1:\n",
        "            best_f1 = v_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train F1: {t_f1:.4f} | Test F1: {v_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "\n",
        "    # --- 실험: AWGN Robustness ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\" EXPERIMENT: Baseline Noise Robustness (Best F1: {best_f1:.4f})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 최고 성능 모델 로드\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    sigma_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "    print(f\"{'Sigma':<10} | {'Accuracy':<10} | {'F1-Score':<10}\")\n",
        "    print(\"-\" * 36)\n",
        "\n",
        "    for sigma in sigma_levels:\n",
        "        acc, f1 = evaluate_with_noise(model, test_loader, DEVICE, sigma=sigma)\n",
        "        print(f\"{sigma:<10.1f} | {acc:<10.4f} | {f1:<10.4f}\")\n",
        "\n",
        "    print(\"-\" * 36)\n",
        "    print(\"Baseline Experiment Completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrCx_fn5lg_E",
        "outputId": "a2d327ce-091a-401e-b6a7-fd6e01b1d09d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Standard 1D-CNN Baseline on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-866237846.py:224: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Training (Standard CNN)...\n",
            "Epoch [10/50] Train F1: 0.9700 | Test F1: 0.9741 (Best: 0.9741)\n",
            "Epoch [20/50] Train F1: 0.9801 | Test F1: 0.9770 (Best: 0.9770)\n",
            "Epoch [30/50] Train F1: 0.9812 | Test F1: 0.9806 (Best: 0.9810)\n",
            "Epoch [40/50] Train F1: 0.9851 | Test F1: 0.9841 (Best: 0.9841)\n",
            "Epoch [50/50] Train F1: 0.9874 | Test F1: 0.9822 (Best: 0.9841)\n",
            "\n",
            "============================================================\n",
            " EXPERIMENT: Baseline Noise Robustness (Best F1: 0.9841)\n",
            "============================================================\n",
            "Sigma      | Accuracy   | F1-Score  \n",
            "------------------------------------\n",
            "0.0        | 0.9847     | 0.9841    \n",
            "0.1        | 0.9845     | 0.9839    \n",
            "0.2        | 0.9842     | 0.9836    \n",
            "0.3        | 0.9842     | 0.9833    \n",
            "0.4        | 0.9825     | 0.9815    \n",
            "0.5        | 0.9816     | 0.9806    \n",
            "------------------------------------\n",
            "Baseline Experiment Completed.\n"
          ]
        }
      ]
    }
  ]
}