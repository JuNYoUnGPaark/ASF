{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 0. Utils & Setup\n",
        "# ------------------------------------------------------------------------------\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. WISDMDataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class WISDMDataset(Dataset):\n",
        "    \"\"\"\n",
        "    단일 WISDM txt 형식:\n",
        "    subject,activity,timestamp,x,y,z;\n",
        "    예) 33,Jogging,49105962326000,-0.6946377,12.680544,0.50395286;\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path: str, window_size: int = 80, step_size: int = 40):\n",
        "        super().__init__()\n",
        "        self.file_path = file_path\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            raise FileNotFoundError(f\"WISDM txt file not found: {file_path}\")\n",
        "\n",
        "        df = self._load_file(file_path)\n",
        "        self.X, self.y, self.subjects = self._create_windows(df)\n",
        "\n",
        "        self.unique_subjects = sorted(np.unique(self.subjects))\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Loaded WISDM dataset (single txt)\")\n",
        "        print(f\"  X shape       : {self.X.shape}  (N, T, C)\")\n",
        "        print(f\"  y shape       : {self.y.shape}  (N,)\")\n",
        "        print(f\"  subjects shape: {self.subjects.shape} (N,)\")\n",
        "        print(f\"  unique subjects: {self.unique_subjects}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def _load_file(self, file_path: str) -> pd.DataFrame:\n",
        "        \"\"\"원본 txt 한 개를 통째로 읽어서 DataFrame으로 변환.\"\"\"\n",
        "        with open(file_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        rows = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            # 끝 세미콜론 제거\n",
        "            line = line.replace(\";\", \"\")\n",
        "            parts = line.split(\",\")\n",
        "\n",
        "            # subject, activity, timestamp, x, y, z → 6개 아니면 스킵\n",
        "            if len(parts) != 6:\n",
        "                continue\n",
        "\n",
        "            subj, act, ts, x, y, z = parts\n",
        "\n",
        "            # x,y,z 중 하나라도 비어있으면 스킵\n",
        "            if x.strip() == \"\" or y.strip() == \"\" or z.strip() == \"\":\n",
        "                continue\n",
        "\n",
        "            rows.append([subj, act, ts, x, y, z])\n",
        "\n",
        "        if not rows:\n",
        "            raise ValueError(f\"No valid rows parsed from file: {file_path}\")\n",
        "\n",
        "        df = pd.DataFrame(rows, columns=[\"subject\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        # 문자열 → NaN 처리 후 숫자로 변환\n",
        "        df = df.replace([\"\", \"NaN\", \"nan\"], np.nan).dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        df[\"subject\"] = pd.to_numeric(df[\"subject\"], errors=\"coerce\")\n",
        "        df[\"x\"] = pd.to_numeric(df[\"x\"], errors=\"coerce\")\n",
        "        df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\")\n",
        "        df[\"z\"] = pd.to_numeric(df[\"z\"], errors=\"coerce\")\n",
        "\n",
        "        df = df.dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        if df.empty:\n",
        "            raise ValueError(\"After cleaning, WISDM DataFrame is empty. Check file format.\")\n",
        "\n",
        "        df[\"subject\"] = df[\"subject\"].astype(int)\n",
        "\n",
        "        # activity 문자열 → 정수 라벨\n",
        "        df[\"activity_id\"] = df[\"activity\"].astype(\"category\").cat.codes\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_windows(self, df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        subject 별로 나눠서 sliding window 생성.\n",
        "        X: (N, T, 3), y: (N,), subjects: (N,)\n",
        "        \"\"\"\n",
        "        X_list, y_list, s_list = [], [], []\n",
        "\n",
        "        for subj_id in sorted(df[\"subject\"].unique()):\n",
        "            df_sub = df[df[\"subject\"] == subj_id]\n",
        "\n",
        "            # 필요하면 여기서 activity별로도 끊을 수 있음 (원하면 확장 가능)\n",
        "            data = df_sub[[\"x\", \"y\", \"z\"]].to_numpy(dtype=np.float32)      # (L, 3)\n",
        "            labels = df_sub[\"activity_id\"].to_numpy(dtype=np.int64)        # (L,)\n",
        "            L = len(df_sub)\n",
        "\n",
        "            start = 0\n",
        "            while start + self.window_size <= L:\n",
        "                end = start + self.window_size\n",
        "\n",
        "                window_x = data[start:end]          # (T, 3)\n",
        "                window_y = labels[end - 1]          # 마지막 타임스텝 라벨\n",
        "\n",
        "                X_list.append(window_x.T)           # (3, T)\n",
        "                y_list.append(window_y)\n",
        "                s_list.append(subj_id)\n",
        "\n",
        "                start += self.step_size\n",
        "\n",
        "        if len(X_list) == 0:\n",
        "            raise ValueError(\"[WISDMDataset] No windows created. Try smaller window_size or check data.\")\n",
        "\n",
        "        X = np.stack(X_list, axis=0).astype(np.float32)  # (N, 3, T)\n",
        "        y = np.array(y_list, dtype=np.int64)\n",
        "        s = np.array(s_list, dtype=np.int64)\n",
        "\n",
        "        # (N, 3, T) → (N, T, 3)\n",
        "        X = X.transpose(0, 2, 1)\n",
        "        return X, y, s\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.FloatTensor(self.X[idx]),          # (T, 3)\n",
        "            torch.LongTensor([self.y[idx]])[0],\n",
        "            self.subjects[idx],\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Baseline Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "# ASF-DCL과 공정한 비교를 위해 동일한 Encoder 구조 사용\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "# Baseline Model: Encoder + Global Average Pooling + Classifier\n",
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64, num_classes=6, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "\n",
        "        # Flow 모듈 없이 바로 분류 (일반적인 CNN 구조)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Encode: [Batch, Time, Dim]\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        # 2. Global Average Pooling (Time 축 평균)\n",
        "        s_pool = torch.mean(s, dim=1)\n",
        "\n",
        "        # 3. Classify\n",
        "        logits = self.classifier(s_pool)\n",
        "        return logits\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Train & Evaluate Functions (Baseline용)\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=0.05)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, f1\n",
        "\n",
        "def evaluate_with_noise(model, dataloader, device, sigma):\n",
        "    \"\"\"\n",
        "    AWGN 노이즈를 주입하여 모델의 견고성을 평가하는 함수\n",
        "    sigma: 노이즈 강도 (Standard Deviation)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            # --- Noise Injection ---\n",
        "            if sigma > 0:\n",
        "                noise = torch.randn_like(x) * sigma\n",
        "                x = x + noise\n",
        "            # -----------------------\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    # 설정 (기존과 동일하게 맞춤)\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data/WISDM_ar_v1.1_raw.txt'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Running Standard 1D-CNN Baseline on {DEVICE}\")\n",
        "\n",
        "    # 데이터 로드\n",
        "    full_dataset = WISDMDataset(DATA_PATH, window_size=80, step_size=40)\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(total_size * 0.8)\n",
        "    test_size = total_size - train_size\n",
        "\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(SEED)\n",
        "    )\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              worker_init_fn=seed_worker, generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = StandardCNN(input_channels=3, latent_dim=64, num_classes=6).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    print(\"\\nStarting Training (Standard CNN)...\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t_loss, t_f1 = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "\n",
        "        # Validation (Noise=0.0)\n",
        "        v_acc, v_f1 = evaluate_with_noise(model, test_loader, DEVICE, sigma=0.0)\n",
        "\n",
        "        if v_f1 > best_f1:\n",
        "            best_f1 = v_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train F1: {t_f1:.4f} | Test F1: {v_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "\n",
        "    # --- 실험: AWGN Robustness ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\" EXPERIMENT: Baseline Noise Robustness (Best F1: {best_f1:.4f})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 최고 성능 모델 로드\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    sigma_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "    print(f\"{'Sigma':<10} | {'Accuracy':<10} | {'F1-Score':<10}\")\n",
        "    print(\"-\" * 36)\n",
        "\n",
        "    for sigma in sigma_levels:\n",
        "        acc, f1 = evaluate_with_noise(model, test_loader, DEVICE, sigma=sigma)\n",
        "        print(f\"{sigma:<10.1f} | {acc:<10.4f} | {f1:<10.4f}\")\n",
        "\n",
        "    print(\"-\" * 36)\n",
        "    print(\"Baseline Experiment Completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrCx_fn5lg_E",
        "outputId": "391dc08f-a5e8-4908-f18f-3b2d8a89316c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Standard 1D-CNN Baseline on cuda\n",
            "================================================================================\n",
            "Loaded WISDM dataset (single txt)\n",
            "  X shape       : (27108, 80, 3)  (N, T, C)\n",
            "  y shape       : (27108,)  (N,)\n",
            "  subjects shape: (27108,) (N,)\n",
            "  unique subjects: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34), np.int64(35), np.int64(36)]\n",
            "================================================================================\n",
            "\n",
            "Starting Training (Standard CNN)...\n",
            "Epoch [10/50] Train F1: 0.9619 | Test F1: 0.9625 (Best: 0.9625)\n",
            "Epoch [20/50] Train F1: 0.9720 | Test F1: 0.9658 (Best: 0.9690)\n",
            "Epoch [30/50] Train F1: 0.9750 | Test F1: 0.9687 (Best: 0.9707)\n",
            "Epoch [40/50] Train F1: 0.9794 | Test F1: 0.9552 (Best: 0.9730)\n",
            "Epoch [50/50] Train F1: 0.9838 | Test F1: 0.9695 (Best: 0.9741)\n",
            "\n",
            "============================================================\n",
            " EXPERIMENT: Baseline Noise Robustness (Best F1: 0.9741)\n",
            "============================================================\n",
            "Sigma      | Accuracy   | F1-Score  \n",
            "------------------------------------\n",
            "0.0        | 0.9847     | 0.9741    \n",
            "0.1        | 0.9843     | 0.9738    \n",
            "0.2        | 0.9840     | 0.9735    \n",
            "0.3        | 0.9840     | 0.9731    \n",
            "0.4        | 0.9847     | 0.9747    \n",
            "0.5        | 0.9828     | 0.9719    \n",
            "------------------------------------\n",
            "Baseline Experiment Completed.\n"
          ]
        }
      ]
    }
  ]
}