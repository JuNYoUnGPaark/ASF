{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 0. Utils & Setup\n",
        "# ------------------------------------------------------------------------------\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. MEHATLH\n",
        "# ------------------------------------------------------------------------------\n",
        "def _load_single_mhealth_log(path: str, feature_cols: list[str]):\n",
        "    \"\"\"\n",
        "    하나의 mHealth_subjectXX.log 파일을 로드해서 DataFrame으로 반환.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(\n",
        "        path,\n",
        "        sep=\"\\t\",\n",
        "        header=None,\n",
        "        names=feature_cols + [\"label\"],\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def load_mhealth_dataframe(data_dir: str):\n",
        "    \"\"\"\n",
        "    data_dir 안의 mHealth_subject*.log 전부 읽어서 하나의 DataFrame으로 concat.\n",
        "    \"\"\"\n",
        "    feature_cols = [\n",
        "        \"acc_chest_x\", \"acc_chest_y\", \"acc_chest_z\",      # 0, 1, 2\n",
        "        \"ecg_1\", \"ecg_2\",                                 # 3, 4\n",
        "        \"acc_ankle_x\", \"acc_ankle_y\", \"acc_ankle_z\",      # 5, 6, 7  <- Physics\n",
        "        \"gyro_ankle_x\", \"gyro_ankle_y\", \"gyro_ankle_z\",   # 8, 9, 10 <- Physics\n",
        "        \"mag_ankle_x\", \"mag_ankle_y\", \"mag_ankle_z\",      # 11, 12, 13\n",
        "        \"acc_arm_x\", \"acc_arm_y\", \"acc_arm_z\",          # 14, 15, 16\n",
        "        \"gyro_arm_x\", \"gyro_arm_y\", \"gyro_arm_z\",       # 17, 18, 19\n",
        "        \"mag_arm_x\", \"mag_arm_y\", \"mag_arm_z\",          # 20, 21, 22\n",
        "    ]  # 총 23 channels\n",
        "\n",
        "    log_files = glob.glob(os.path.join(data_dir, \"mHealth_subject*.log\"))\n",
        "    if not log_files:\n",
        "         raise FileNotFoundError(f\"No mHealth_subject*.log files found in {data_dir}\")\n",
        "    print(f\"Found {len(log_files)} log files in {data_dir}\")\n",
        "\n",
        "    dfs = []\n",
        "    for fp in log_files:\n",
        "        df_i = _load_single_mhealth_log(fp, feature_cols)\n",
        "        dfs.append(df_i)\n",
        "\n",
        "    full_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Null 클래스(label==0)는 제외\n",
        "    full_df = full_df[full_df[\"label\"] != 0].copy()\n",
        "\n",
        "    # 원래 라벨 1~12 → 0~11 로 shift\n",
        "    full_df.loc[:, \"label\"] = full_df[\"label\"] - 1\n",
        "\n",
        "    return full_df, feature_cols\n",
        "\n",
        "def create_mhealth_windows(\n",
        "    df: pd.DataFrame,\n",
        "    feature_cols: list[str],\n",
        "    window_size: int,\n",
        "    step_size: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    전체 시계열을 (window_size, step_size)로 슬라이딩하면서 윈도우 생성.\n",
        "    반환:\n",
        "        X_np : (N, C, T) float32  <- (채널, 시간)\n",
        "        y_np : (N,) int64         <- 0~11 로 이미 shift된 라벨\n",
        "    \"\"\"\n",
        "    data_arr = df[feature_cols].to_numpy(dtype=np.float32)  # (L, 23)\n",
        "    labels_arr = df[\"label\"].to_numpy(dtype=np.int64)       # (L,)\n",
        "    L = data_arr.shape[0]\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    start = 0\n",
        "    while start + window_size <= L:\n",
        "        end = start + window_size\n",
        "        window_x = data_arr[start:end]        # (T, C)\n",
        "        window_label = labels_arr[end - 1]    # 마지막 타임스텝 라벨\n",
        "        window_x_ct = np.transpose(window_x, (1, 0))  # (C, T)\n",
        "\n",
        "        X_list.append(window_x_ct)\n",
        "        y_list.append(int(window_label))\n",
        "        start += step_size\n",
        "\n",
        "    if not X_list:\n",
        "        raise RuntimeError(\"No windows created. Check window_size / step_size / dataset length.\")\n",
        "\n",
        "    X_np = np.stack(X_list, axis=0).astype(np.float32)  # (N, C, T)\n",
        "    y_np = np.array(y_list, dtype=np.int64)             # (N,)\n",
        "    return X_np, y_np\n",
        "\n",
        "class MHEALTHDataset(Dataset):\n",
        "    \"\"\"\n",
        "    MHEALTH Dataset wrapper\n",
        "    - X를 (N, T, C) 형태로 저장\n",
        "    - __getitem__이 (X_tc, y, s) 튜플을 반환하도록 수정\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir: str, window_size: int = 128, step_size: int = 64):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) 로그 로드 & 전처리\n",
        "        full_df, feature_cols = load_mhealth_dataframe(data_dir)\n",
        "\n",
        "        # 2) 슬라이딩 윈도우 생성\n",
        "        X, y = create_mhealth_windows(\n",
        "            df=full_df,\n",
        "            feature_cols=feature_cols,\n",
        "            window_size=window_size,\n",
        "            step_size=step_size,\n",
        "        ) # X: (N, C, T), y: (N,)\n",
        "\n",
        "        # X를 (N, T, C) 형태로 저장 (UCI-HAR와 통일)\n",
        "        self.X = np.transpose(X, (0, 2, 1)).astype(np.float32)\n",
        "        self.y = y\n",
        "        # 더미 subject ID 추가 (collate_fn 호환용)\n",
        "        self.subjects = np.zeros(len(self.y), dtype=int) # (기존 코드 호환용)\n",
        "\n",
        "        # 라벨 이름 (0~11 인덱스 기준)\n",
        "        self.label_names = [\n",
        "            \"Standing still\", \"Sitting and relaxing\", \"Lying down\",\n",
        "            \"Walking\", \"Climbing stairs\", \"Waist bends forward\",\n",
        "            \"Frontal elevation of arms\", \"Knees bending\", \"Cycling\",\n",
        "            \"Jogging\", \"Running\", \"Jump front & back\",\n",
        "        ]\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Loaded MHEALTH dataset\")\n",
        "        print(f\"  X shape : {self.X.shape}  (N, T, C)\")\n",
        "        print(f\"  y shape : {self.y.shape}  (N,)\")\n",
        "        print(f\"  Classes : {len(self.label_names)}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"\n",
        "        (T, C) 텐서, 라벨, 더미 서브젝트 ID 반환\n",
        "        \"\"\"\n",
        "        return (torch.FloatTensor(self.X[idx]),       # (T, C)\n",
        "                torch.LongTensor([self.y[idx]])[0], # scalar\n",
        "                self.subjects[idx])                 # scalar (dummy)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Baseline Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "# ASF-DCL과 공정한 비교를 위해 동일한 Encoder 구조 사용\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "# Baseline Model: Encoder + Global Average Pooling + Classifier\n",
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64, num_classes=6, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "\n",
        "        # Flow 모듈 없이 바로 분류 (일반적인 CNN 구조)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Encode: [Batch, Time, Dim]\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        # 2. Global Average Pooling (Time 축 평균)\n",
        "        s_pool = torch.mean(s, dim=1)\n",
        "\n",
        "        # 3. Classify\n",
        "        logits = self.classifier(s_pool)\n",
        "        return logits\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Train & Evaluate Functions (Baseline용)\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=0.05)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, f1\n",
        "\n",
        "def evaluate_with_noise(model, dataloader, device, sigma):\n",
        "    \"\"\"\n",
        "    AWGN 노이즈를 주입하여 모델의 견고성을 평가하는 함수\n",
        "    sigma: 노이즈 강도 (Standard Deviation)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            # --- Noise Injection ---\n",
        "            if sigma > 0:\n",
        "                noise = torch.randn_like(x) * sigma\n",
        "                x = x + noise\n",
        "            # -----------------------\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    # 설정 (기존과 동일하게 맞춤)\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Running Standard 1D-CNN Baseline on {DEVICE}\")\n",
        "\n",
        "    # 데이터 로드\n",
        "    full_dataset = MHEALTHDataset(DATA_PATH, window_size=128, step_size=64)\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(total_size * 0.8)\n",
        "    test_size = total_size - train_size\n",
        "\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(SEED)\n",
        "    )\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              worker_init_fn=seed_worker, generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = StandardCNN(input_channels=23, latent_dim=64, num_classes=12).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    print(\"\\nStarting Training (Standard CNN)...\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t_loss, t_f1 = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "\n",
        "        # Validation (Noise=0.0)\n",
        "        v_acc, v_f1 = evaluate_with_noise(model, test_loader, DEVICE, sigma=0.0)\n",
        "\n",
        "        if v_f1 > best_f1:\n",
        "            best_f1 = v_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train F1: {t_f1:.4f} | Test F1: {v_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "\n",
        "    # --- 실험: AWGN Robustness ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\" EXPERIMENT: Baseline Noise Robustness (Best F1: {best_f1:.4f})\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 최고 성능 모델 로드\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    sigma_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "    print(f\"{'Sigma':<10} | {'Accuracy':<10} | {'F1-Score':<10}\")\n",
        "    print(\"-\" * 36)\n",
        "\n",
        "    for sigma in sigma_levels:\n",
        "        acc, f1 = evaluate_with_noise(model, test_loader, DEVICE, sigma=sigma)\n",
        "        print(f\"{sigma:<10.1f} | {acc:<10.4f} | {f1:<10.4f}\")\n",
        "\n",
        "    print(\"-\" * 36)\n",
        "    print(\"Baseline Experiment Completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrCx_fn5lg_E",
        "outputId": "44622b31-e289-4d8d-89b6-847311417f6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Standard 1D-CNN Baseline on cuda\n",
            "Found 10 log files in /content/drive/MyDrive/Colab Notebooks/HAR_data/MHEALTHDATASET\n",
            "================================================================================\n",
            "Loaded MHEALTH dataset\n",
            "  X shape : (5361, 128, 23)  (N, T, C)\n",
            "  y shape : (5361,)  (N,)\n",
            "  Classes : 12\n",
            "================================================================================\n",
            "\n",
            "Starting Training (Standard CNN)...\n",
            "Epoch [10/50] Train F1: 0.9785 | Test F1: 0.9739 (Best: 0.9766)\n",
            "Epoch [20/50] Train F1: 0.9815 | Test F1: 0.9774 (Best: 0.9774)\n",
            "Epoch [30/50] Train F1: 0.9859 | Test F1: 0.9756 (Best: 0.9774)\n",
            "Epoch [40/50] Train F1: 0.9871 | Test F1: 0.9766 (Best: 0.9774)\n",
            "Epoch [50/50] Train F1: 0.9885 | Test F1: 0.9736 (Best: 0.9774)\n",
            "\n",
            "============================================================\n",
            " EXPERIMENT: Baseline Noise Robustness (Best F1: 0.9774)\n",
            "============================================================\n",
            "Sigma      | Accuracy   | F1-Score  \n",
            "------------------------------------\n",
            "0.0        | 0.9814     | 0.9774    \n",
            "0.1        | 0.9814     | 0.9774    \n",
            "0.2        | 0.9814     | 0.9774    \n",
            "0.3        | 0.9814     | 0.9774    \n",
            "0.4        | 0.9814     | 0.9774    \n",
            "0.5        | 0.9814     | 0.9774    \n",
            "------------------------------------\n",
            "Baseline Experiment Completed.\n"
          ]
        }
      ]
    }
  ]
}