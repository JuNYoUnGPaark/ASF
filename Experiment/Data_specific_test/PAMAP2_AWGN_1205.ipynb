{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import copy\n",
        "import glob\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. PAMAP2 Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "def create_pamap2_windows(df: pd.DataFrame, window_size: int, step_size: int):\n",
        "    \"\"\"\n",
        "    subject별로 timestamp 순서대로 전체 시계열을 따라가며 슬라이딩 윈도우 생성.\n",
        "    한 윈도우의 라벨은 마지막 프레임의 activityID.\n",
        "    마지막 라벨이 0(Null/기타) 이면 그 윈도우는 버린다.\n",
        "\n",
        "    Returns:\n",
        "        X:          (N, C, T) float32\n",
        "        y:          (N,) int64  (0..11로 리맵된 레이블)\n",
        "        subj_ids:   (N,) int64\n",
        "        label_names:list[str] 길이 12, new_index -> human-readable\n",
        "    \"\"\"\n",
        "\n",
        "    # 사용할 피처들 (orientation*, heartrate, *_Temperature 등은 제외)\n",
        "    feature_cols = [\n",
        "        # hand\n",
        "        \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "        \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "        \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "        \"handMagne1\",\"handMagne2\",\"handMagne3\",\n",
        "        # chest\n",
        "        \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "        \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "        \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "        \"chestMagne1\",\"chestMagne2\",\"chestMagne3\",\n",
        "        # ankle\n",
        "        \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "        \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "        \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "        \"ankleMagne1\",\"ankleMagne2\",\"ankleMagne3\",\n",
        "    ]\n",
        "\n",
        "    # PAMAP2 실제 activityID들 중 우리가 쓰는 12개 클래스만 남김\n",
        "    # 순서 고정: 이 순서가 new class index 0..11이 된다.\n",
        "    ORDERED_IDS = [1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24]\n",
        "\n",
        "    # 원본 activityID -> new index(0..11)\n",
        "    old2new = {\n",
        "        1: 0,   # Lying\n",
        "        2: 1,   # Sitting\n",
        "        3: 2,   # Standing\n",
        "        4: 3,   # Walking\n",
        "        5: 4,   # Running\n",
        "        6: 5,   # Cycling\n",
        "        7: 6,   # Nordic walking\n",
        "        12: 7,  # Ascending stairs\n",
        "        13: 8,  # Descending stairs\n",
        "        16: 9,  # Vacuum cleaning\n",
        "        17: 10, # Ironing\n",
        "        24: 11, # Rope jumping\n",
        "    }\n",
        "\n",
        "    # new index -> 사람이 읽는 이름\n",
        "    label_names = [\n",
        "        \"Lying\",              # 0 -> orig 1\n",
        "        \"Sitting\",            # 1 -> orig 2\n",
        "        \"Standing\",           # 2 -> orig 3\n",
        "        \"Walking\",            # 3 -> orig 4\n",
        "        \"Running\",            # 4 -> orig 5\n",
        "        \"Cycling\",            # 5 -> orig 6\n",
        "        \"Nordic walking\",     # 6 -> orig 7\n",
        "        \"Ascending stairs\",   # 7 -> orig 12\n",
        "        \"Descending stairs\",  # 8 -> orig 13\n",
        "        \"Vacuum cleaning\",    # 9 -> orig 16\n",
        "        \"Ironing\",            # 10 -> orig 17\n",
        "        \"Rope jumping\",       # 11 -> orig 24\n",
        "    ]\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    subj_list = []\n",
        "\n",
        "    # subject별로 끊어서 시간 순 정렬 후 슬라이딩 윈도우\n",
        "    for subj_id, g in df.groupby(\"subject_id\"):\n",
        "        # 시간순 정렬\n",
        "        if \"timestamp\" in g.columns:\n",
        "            g = g.sort_values(\"timestamp\")\n",
        "        else:\n",
        "            g = g.sort_index()\n",
        "\n",
        "        data_arr  = g[feature_cols].to_numpy(dtype=np.float32)   # (L, C)\n",
        "        label_arr = g[\"activityID\"].to_numpy(dtype=np.int64)     # (L,)\n",
        "        L = data_arr.shape[0]\n",
        "\n",
        "        start = 0\n",
        "        while start + window_size <= L:\n",
        "            end = start + window_size\n",
        "\n",
        "            last_label_orig = int(label_arr[end - 1])\n",
        "\n",
        "            # 0 = \"other / null\" → 버림\n",
        "            if last_label_orig == 0:\n",
        "                start += step_size\n",
        "                continue\n",
        "\n",
        "            # 우리가 쓰는 12개 클래스에 없는 라벨이면 버림\n",
        "            if last_label_orig not in old2new:\n",
        "                start += step_size\n",
        "                continue\n",
        "\n",
        "            # 윈도우 추출\n",
        "            window_ct = data_arr[start:end].T  # (T, C) -> (C, T)\n",
        "\n",
        "            X_list.append(window_ct)\n",
        "            y_list.append(old2new[last_label_orig])\n",
        "            subj_list.append(int(subj_id))\n",
        "\n",
        "            start += step_size\n",
        "\n",
        "    # numpy 변환\n",
        "    X = np.stack(X_list, axis=0).astype(np.float32)      # (N, C, T)\n",
        "    y = np.asarray(y_list, dtype=np.int64)               # (N,)\n",
        "    subj_ids = np.asarray(subj_list, dtype=np.int64)     # (N,)\n",
        "\n",
        "    return X, y, subj_ids, label_names\n",
        "\n",
        "class PAMAP2Dataset(Dataset):\n",
        "    def __init__(self, data_dir, window_size, step_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1) CSV 전부 읽어서 하나의 df로 합치기\n",
        "        csv_files = glob.glob(os.path.join(data_dir, \"*.csv\"))\n",
        "        if len(csv_files) == 0:\n",
        "            raise RuntimeError(f\"No CSV files found under {data_dir}\")\n",
        "\n",
        "        dfs = []\n",
        "        for fpath in sorted(csv_files):\n",
        "            df_i = pd.read_csv(fpath)\n",
        "\n",
        "            if \"subject_id\" not in df_i.columns:\n",
        "                m = re.findall(r\"\\d+\", os.path.basename(fpath))\n",
        "                subj_guess = int(m[0]) if len(m) > 0 else 0\n",
        "                df_i[\"subject_id\"] = subj_guess\n",
        "\n",
        "            dfs.append(df_i)\n",
        "\n",
        "        df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "        df = df.dropna(subset=['activityID'])\n",
        "\n",
        "        # 기본 타입 정리\n",
        "        df[\"activityID\"] = df[\"activityID\"].astype(np.int64)\n",
        "        df[\"subject_id\"] = df[\"subject_id\"].astype(np.int64)\n",
        "        if \"timestamp\" in df.columns:\n",
        "            df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "        # ===========================\n",
        "        # (1) NaN 처리\n",
        "        # ===========================\n",
        "        feature_cols = [\n",
        "            # hand\n",
        "            \"handAcc16_1\",\"handAcc16_2\",\"handAcc16_3\",\n",
        "            \"handAcc6_1\",\"handAcc6_2\",\"handAcc6_3\",\n",
        "            \"handGyro1\",\"handGyro2\",\"handGyro3\",\n",
        "            \"handMagne1\",\"handMagne2\",\"handMagne3\",\n",
        "            # chest\n",
        "            \"chestAcc16_1\",\"chestAcc16_2\",\"chestAcc16_3\",\n",
        "            \"chestAcc6_1\",\"chestAcc6_2\",\"chestAcc6_3\",\n",
        "            \"chestGyro1\",\"chestGyro2\",\"chestGyro3\",\n",
        "            \"chestMagne1\",\"chestMagne2\",\"chestMagne3\",\n",
        "            # ankle\n",
        "            \"ankleAcc16_1\",\"ankleAcc16_2\",\"ankleAcc16_3\",\n",
        "            \"ankleAcc6_1\",\"ankleAcc6_2\",\"ankleAcc6_3\",\n",
        "            \"ankleGyro1\",\"ankleGyro2\",\"ankleGyro3\",\n",
        "            \"ankleMagne1\",\"ankleMagne2\",\"ankleMagne3\",\n",
        "        ]\n",
        "\n",
        "        # subject별로 결측치 보간 -> ffill/bfill로 마저 메우기\n",
        "        def _fill_subject_group(g):\n",
        "            # 시간 순으로 정렬 (timestamp 있으면 timestamp 기준)\n",
        "            if \"timestamp\" in g.columns:\n",
        "                g = g.sort_values(\"timestamp\")\n",
        "            else:\n",
        "                g = g.sort_index()\n",
        "\n",
        "            # 각 컬럼별로 interpolate + ffill/bfill\n",
        "            g[feature_cols] = (\n",
        "                g[feature_cols]\n",
        "                .interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n",
        "                .ffill()\n",
        "                .bfill()\n",
        "            )\n",
        "            return g\n",
        "\n",
        "        df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n",
        "\n",
        "        # 이 시점에서 feature_cols 안에 NaN이 남아있으면 안 됨\n",
        "        # 혹시라도 남았으면 0으로 막아버리기 (safety net)\n",
        "        df[feature_cols] = df[feature_cols].fillna(0.0)\n",
        "\n",
        "        # ===========================\n",
        "        # (2) 스케일 표준화\n",
        "        # ===========================\n",
        "        scaler = StandardScaler()\n",
        "        df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "        # ===========================\n",
        "        # (3) 윈도우 생성\n",
        "        # ===========================\n",
        "        X, y, subj_ids, label_names = create_pamap2_windows(\n",
        "            df,\n",
        "            window_size=window_size,\n",
        "            step_size=step_size,\n",
        "        )\n",
        "\n",
        "        self.X = np.transpose(X, (0, 2, 1)).astype(np.float32)\n",
        "        self.y = y\n",
        "        self.subject_ids = subj_ids\n",
        "        self.label_names = label_names\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.from_numpy(self.X[idx]).float(),\n",
        "            torch.tensor(self.y[idx], dtype=torch.long),\n",
        "            self.subject_ids[idx]\n",
        "        )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. ASF Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "class FlowComputer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, s):\n",
        "        B, T, D = s.shape\n",
        "\n",
        "        flow_raw = s[:, 1:, :] - s[:, :-1, :]\n",
        "        flow_mag = torch.norm(flow_raw, dim=-1, keepdim=True)\n",
        "        flow_dir = flow_raw / (flow_mag + 1e-8)\n",
        "\n",
        "        flow_features = torch.cat(\n",
        "            [flow_raw, flow_mag.expand(-1, -1, D), flow_dir],\n",
        "            dim=-1\n",
        "        )\n",
        "        return flow_features, flow_raw, flow_mag\n",
        "\n",
        "class FlowEncoder(nn.Module):\n",
        "    def __init__(self, flow_dim, hidden_dim=64, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.flow_embed = nn.Linear(flow_dim, hidden_dim)\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.flow_conv1 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.flow_conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=1, padding=0)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "    def forward(self, flow_features):\n",
        "        h = self.flow_embed(flow_features)\n",
        "        h_att, _ = self.attention(h, h, h)\n",
        "\n",
        "        h_att = h_att.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.flow_conv1(h_att)))\n",
        "        h = F.relu(self.bn2(self.flow_conv2(h)))\n",
        "\n",
        "        h_pool = torch.mean(h, dim=-1)\n",
        "        return h_pool\n",
        "\n",
        "class StateTransitionPredictor(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, s_t):\n",
        "        B, Tm1, D = s_t.shape\n",
        "        inp = s_t.reshape(B * Tm1, D)\n",
        "        out = self.net(inp)\n",
        "        return out.reshape(B, Tm1, D)\n",
        "\n",
        "class ASFDCLClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_channels=9,\n",
        "                 latent_dim=64,\n",
        "                 hidden_dim=64,\n",
        "                 num_classes=6,\n",
        "                 num_heads=4,\n",
        "                 projection_dim=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "        self.flow_computer = FlowComputer()\n",
        "        self.flow_encoder = FlowEncoder(latent_dim * 3, hidden_dim, num_heads)\n",
        "        self.state_predictor = StateTransitionPredictor(latent_dim, hidden_dim)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "        self.flow_prototypes = nn.Parameter(\n",
        "            torch.randn(num_classes, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.projection_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, projection_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_details=False):\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        s_t = s[:, :-1, :]\n",
        "        s_next = s[:, 1:, :]\n",
        "        s_pred_next = self.state_predictor(s_t)\n",
        "\n",
        "        flow_features, flow_raw, flow_mag = self.flow_computer(s)\n",
        "\n",
        "        h = self.flow_encoder(flow_features)\n",
        "\n",
        "        z = self.projection_head(h)\n",
        "        z = F.normalize(z, dim=-1)\n",
        "\n",
        "        logits = self.classifier(h)\n",
        "\n",
        "        if not return_details:\n",
        "            return logits\n",
        "\n",
        "        details = {\n",
        "            \"s\": s,\n",
        "            \"s_t\": s_t,\n",
        "            \"s_next\": s_next,\n",
        "            \"s_pred_next\": s_pred_next,\n",
        "            \"flow_features\": flow_features,\n",
        "            \"flow_raw\": flow_raw,\n",
        "            \"flow_mag\": flow_mag,\n",
        "            \"h\": h,\n",
        "            \"z\": z,\n",
        "            \"prototypes\": self.flow_prototypes\n",
        "        }\n",
        "        return logits, details\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Dynamics-aware Contrastive Loss\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_contrastive_loss(z, labels, temperature=0.07):\n",
        "    B = z.shape[0]\n",
        "    device = z.device\n",
        "\n",
        "    sim_matrix = torch.mm(z, z.t()) / temperature\n",
        "\n",
        "    labels_expanded = labels.unsqueeze(1)\n",
        "    positive_mask = (labels_expanded == labels_expanded.t()).float()\n",
        "\n",
        "    positive_mask = positive_mask - torch.eye(B, device=device)\n",
        "\n",
        "    mask = torch.eye(B, device=device).bool()\n",
        "    sim_matrix_masked = sim_matrix.masked_fill(mask, float('-inf'))\n",
        "\n",
        "    exp_sim = torch.exp(sim_matrix_masked)\n",
        "\n",
        "    pos_sim = (exp_sim * positive_mask).sum(dim=1)\n",
        "\n",
        "    all_sim = exp_sim.sum(dim=1)\n",
        "\n",
        "    has_positive = positive_mask.sum(dim=1) > 0\n",
        "\n",
        "    if has_positive.sum() == 0:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "\n",
        "    loss = -torch.log(pos_sim[has_positive] / (all_sim[has_positive] + 1e-8))\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. ASF-DCL Losses: CE + L_dyn + L_flow_prior + L_proto + L_contrast\n",
        "# ------------------------------------------------------------------------------\n",
        "def compute_asf_dcl_losses(logits, details, labels,\n",
        "                           lambda_dyn=0.1,\n",
        "                           lambda_flow=0.05,\n",
        "                           lambda_proto=0.1,\n",
        "                           lambda_contrast=0.15,\n",
        "                           dyn_classes=(0, 1, 2),\n",
        "                           static_classes=(3, 4, 5),\n",
        "                           dyn_target=0.7,\n",
        "                           static_target=0.1,\n",
        "                           proto_tau=0.1,\n",
        "                           contrast_temp=0.07):\n",
        "    device = logits.device\n",
        "\n",
        "    cls_loss = F.cross_entropy(logits, labels, label_smoothing=0.05)\n",
        "\n",
        "    s_next = details[\"s_next\"]\n",
        "    s_pred_next = details[\"s_pred_next\"]\n",
        "    dyn_loss = F.mse_loss(s_pred_next, s_next)\n",
        "\n",
        "    flow_mag = details[\"flow_mag\"]\n",
        "    B, Tm1, _ = flow_mag.shape\n",
        "    flow_mean = flow_mag.mean(dim=1).view(B)\n",
        "\n",
        "    dyn_mask = torch.zeros_like(flow_mean, dtype=torch.bool)\n",
        "    static_mask = torch.zeros_like(flow_mean, dtype=torch.bool)\n",
        "    for c in dyn_classes:\n",
        "        dyn_mask = dyn_mask | (labels == c)\n",
        "    for c in static_classes:\n",
        "        static_mask = static_mask | (labels == c)\n",
        "\n",
        "    flow_prior_loss = torch.tensor(0.0, device=device)\n",
        "    if dyn_mask.any():\n",
        "        dyn_flow = flow_mean[dyn_mask]\n",
        "        flow_prior_loss = flow_prior_loss + F.mse_loss(\n",
        "            dyn_flow, torch.full_like(dyn_flow, dyn_target)\n",
        "        )\n",
        "    if static_mask.any():\n",
        "        static_flow = flow_mean[static_mask]\n",
        "        flow_prior_loss = flow_prior_loss + F.mse_loss(\n",
        "            static_flow, torch.full_like(static_flow, static_target)\n",
        "        )\n",
        "\n",
        "    h = details[\"h\"]\n",
        "    prototypes = details[\"prototypes\"]\n",
        "\n",
        "    h_norm = F.normalize(h, dim=-1)\n",
        "    proto_norm = F.normalize(prototypes, dim=-1)\n",
        "\n",
        "    sim = h_norm @ proto_norm.t()\n",
        "    proto_logits = sim / proto_tau\n",
        "    proto_loss = F.cross_entropy(proto_logits, labels, label_smoothing=0.05)\n",
        "\n",
        "    z = details[\"z\"]\n",
        "    contrast_loss = compute_contrastive_loss(z, labels, temperature=contrast_temp)\n",
        "\n",
        "    total_loss = (\n",
        "        cls_loss +\n",
        "        lambda_dyn * dyn_loss +\n",
        "        lambda_flow * flow_prior_loss +\n",
        "        lambda_proto * proto_loss +\n",
        "        lambda_contrast * contrast_loss\n",
        "    )\n",
        "\n",
        "    loss_dict = {\n",
        "        \"total\": total_loss.item(),\n",
        "        \"cls\": cls_loss.item(),\n",
        "        \"dyn\": dyn_loss.item(),\n",
        "        \"flow_prior\": flow_prior_loss.item(),\n",
        "        \"proto\": proto_loss.item(),\n",
        "        \"contrast\": contrast_loss.item()\n",
        "    }\n",
        "    return total_loss, loss_dict\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Train / Evaluation\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device,\n",
        "                lambda_dyn=0.1, lambda_flow=0.05,\n",
        "                lambda_proto=0.1, lambda_contrast=0.15, **kwargs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    loss_accumulator = {\n",
        "        \"cls\": 0.0,\n",
        "        \"dyn\": 0.0,\n",
        "        \"flow_prior\": 0.0,\n",
        "        \"proto\": 0.0,\n",
        "        \"contrast\": 0.0\n",
        "    }\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x, y = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits, details = model(x, return_details=True)\n",
        "        loss, loss_dict = compute_asf_dcl_losses(\n",
        "            logits, details, y,\n",
        "            lambda_dyn=lambda_dyn,\n",
        "            lambda_flow=lambda_flow,\n",
        "            lambda_proto=lambda_proto,\n",
        "            lambda_contrast=lambda_contrast,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        for k in loss_accumulator.keys():\n",
        "            loss_accumulator[k] += loss_dict[k]\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    for k in loss_accumulator.keys():\n",
        "        loss_accumulator[k] /= len(dataloader)\n",
        "\n",
        "    return avg_loss, acc, f1, loss_accumulator\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x, y = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return avg_loss, acc, f1, cm\n",
        "\n",
        "def evaluate_with_noise(model, dataloader, device, sigma):\n",
        "    \"\"\"\n",
        "    AWGN(Additive White Gaussian Noise) 실험을 위한 평가 함수\n",
        "    sigma: 노이즈의 표준편차 (강도)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x, y = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "            # --- [핵심 수정 부분] 노이즈 주입 ---\n",
        "            if sigma > 0:\n",
        "                # Random Normal(0, 1) 생성 후 sigma 곱하기\n",
        "                noise = torch.randn_like(x) * sigma\n",
        "                x = x + noise\n",
        "            # ----------------------------------\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "    return acc, f1\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Main Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    INPUT_CHANNELS = 36\n",
        "    NUM_CLASSES = 12\n",
        "\n",
        "    STATIC_CLS = (0, 1, 2)\n",
        "    DYN_CLS = (3, 4, 5, 6, 7, 8, 9, 10, 11)\n",
        "\n",
        "    LAMBDA_DYN = 0.05\n",
        "    LAMBDA_FLOW = 0.02\n",
        "    LAMBDA_PROTO = 0.05\n",
        "    LAMBDA_CONTRAST = 0.2\n",
        "\n",
        "    print(\"-\"*80)\n",
        "    print(\"ASF-DCL: Action State Flow with Dynamics-aware Contrastive Learning\")\n",
        "    print(\"-\"*80)\n",
        "    print(f\"Batch size: {BATCH_SIZE}\")\n",
        "    print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "    print(f\"Learning rate: {LEARNING_RATE}\")\n",
        "    print()\n",
        "    print(f\"Lambda_dyn: {LAMBDA_DYN}\")\n",
        "    print(f\"Lambda_flow:  {LAMBDA_FLOW}\")\n",
        "    print(f\"Lambda_proto: {LAMBDA_PROTO}\")\n",
        "    print(f\"Lambda_contrast: {LAMBDA_CONTRAST}\")\n",
        "    print()\n",
        "\n",
        "    try:\n",
        "        full_dataset = PAMAP2Dataset(DATA_PATH, window_size=128, step_size=64)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, num_workers=2,\n",
        "                              worker_init_fn=seed_worker,\n",
        "                              generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, num_workers=2,\n",
        "                             worker_init_fn=seed_worker,\n",
        "                             generator=g)\n",
        "\n",
        "    model = ASFDCLClassifier(\n",
        "        input_channels=INPUT_CHANNELS,\n",
        "        latent_dim=64,\n",
        "        hidden_dim=64,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        num_heads=4,\n",
        "        projection_dim=128\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print()\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=LEARNING_RATE,\n",
        "                                 weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"TRAINING\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, train_f1, loss_dict = train_epoch(\n",
        "            model, train_loader, optimizer, DEVICE,\n",
        "            lambda_dyn=LAMBDA_DYN,\n",
        "            lambda_flow=LAMBDA_FLOW,\n",
        "            lambda_proto=LAMBDA_PROTO,\n",
        "            lambda_contrast=LAMBDA_CONTRAST,\n",
        "            dyn_classes=DYN_CLS,\n",
        "            static_classes=STATIC_CLS\n",
        "        )\n",
        "\n",
        "        test_loss, test_acc, test_f1, test_cm = evaluate(\n",
        "            model, test_loader, DEVICE\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if test_f1 > best_f1:\n",
        "            best_f1 = test_f1\n",
        "            best_acc = test_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        best_acc = max(best_acc, test_acc)\n",
        "        best_f1 = max(best_f1, test_f1)\n",
        "\n",
        "        log_msg = (f\"[{epoch+1:02d}/{NUM_EPOCHS}] \"\n",
        "                   f\"Train Loss: {train_loss:.3f} | F1: {train_f1:.4f}  |  \"\n",
        "                   f\"Test F1: {test_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "        print(log_msg)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"EVALUATION...\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    test_loss, test_acc, test_f1, test_cm = evaluate(\n",
        "        model, test_loader, DEVICE\n",
        "    )\n",
        "    print(f\"Final Result → Best Test F1: {best_f1:.4f} (Acc: {best_acc:.4f})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" EXPERIMENT: AWGN Robustness\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Load the Best Model\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    print(f\"Loaded Best Model (F1: {best_f1:.4f}) for testing...\")\n",
        "\n",
        "    # 2. Define Bias Levels (c)\n",
        "    # Testing bias from 0.0 to 0.5 as suggested in robustneㄴss experiments\n",
        "    sigma_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "\n",
        "    print(f\"\\n{'Sigma (c)':<10} | {'Accuracy':<10} | {'F1-Score':<10}\")\n",
        "    print(\"-\" * 36)\n",
        "\n",
        "    # 3. Run Test for each Bias level\n",
        "    results = {}\n",
        "    for sigma in sigma_levels:\n",
        "        acc, f1 = evaluate_with_noise(model, test_loader, DEVICE, sigma=sigma)\n",
        "        results[sigma] = f1\n",
        "        print(f\"{sigma:<10.1f} | {acc:<10.4f} | {f1:<10.4f}\")\n",
        "\n",
        "    print(\"-\" * 36)\n",
        "    print(\"AWGN Noise Drift Experiment Completed.\")\n",
        "\n",
        "    return model, best_acc, best_f1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrCx_fn5lg_E",
        "outputId": "4228aa1f-682f-4bf7-f200-64a16617cde1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "ASF-DCL: Action State Flow with Dynamics-aware Contrastive Learning\n",
            "--------------------------------------------------------------------------------\n",
            "Batch size: 64\n",
            "Epochs: 50\n",
            "Learning rate: 0.001\n",
            "\n",
            "Lambda_dyn: 0.05\n",
            "Lambda_flow:  0.02\n",
            "Lambda_proto: 0.05\n",
            "Lambda_contrast: 0.2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2405313648.py:218: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby(\"subject_id\", group_keys=False).apply(_fill_subject_group)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total parameters: 101,036\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TRAINING\n",
            "--------------------------------------------------------------------------------\n",
            "[01/50] Train Loss: 1.420 | F1: 0.6649  |  Test F1: 0.7536 (Best: 0.7536)\n",
            "[02/50] Train Loss: 0.888 | F1: 0.8553  |  Test F1: 0.8048 (Best: 0.8048)\n",
            "[03/50] Train Loss: 0.775 | F1: 0.8883  |  Test F1: 0.9034 (Best: 0.9034)\n",
            "[04/50] Train Loss: 0.712 | F1: 0.9031  |  Test F1: 0.9006 (Best: 0.9034)\n",
            "[05/50] Train Loss: 0.645 | F1: 0.9183  |  Test F1: 0.9168 (Best: 0.9168)\n",
            "[06/50] Train Loss: 0.618 | F1: 0.9238  |  Test F1: 0.9251 (Best: 0.9251)\n",
            "[07/50] Train Loss: 0.599 | F1: 0.9275  |  Test F1: 0.8954 (Best: 0.9251)\n",
            "[08/50] Train Loss: 0.566 | F1: 0.9372  |  Test F1: 0.9316 (Best: 0.9316)\n",
            "[09/50] Train Loss: 0.550 | F1: 0.9382  |  Test F1: 0.9347 (Best: 0.9347)\n",
            "[10/50] Train Loss: 0.536 | F1: 0.9416  |  Test F1: 0.9350 (Best: 0.9350)\n",
            "[11/50] Train Loss: 0.518 | F1: 0.9468  |  Test F1: 0.9405 (Best: 0.9405)\n",
            "[12/50] Train Loss: 0.514 | F1: 0.9468  |  Test F1: 0.9438 (Best: 0.9438)\n",
            "[13/50] Train Loss: 0.499 | F1: 0.9488  |  Test F1: 0.9381 (Best: 0.9438)\n",
            "[14/50] Train Loss: 0.483 | F1: 0.9545  |  Test F1: 0.9451 (Best: 0.9451)\n",
            "[15/50] Train Loss: 0.474 | F1: 0.9574  |  Test F1: 0.9384 (Best: 0.9451)\n",
            "[16/50] Train Loss: 0.472 | F1: 0.9567  |  Test F1: 0.9405 (Best: 0.9451)\n",
            "[17/50] Train Loss: 0.457 | F1: 0.9645  |  Test F1: 0.9513 (Best: 0.9513)\n",
            "[18/50] Train Loss: 0.447 | F1: 0.9643  |  Test F1: 0.9414 (Best: 0.9513)\n",
            "[19/50] Train Loss: 0.441 | F1: 0.9674  |  Test F1: 0.9544 (Best: 0.9544)\n",
            "[20/50] Train Loss: 0.433 | F1: 0.9705  |  Test F1: 0.9593 (Best: 0.9593)\n",
            "[21/50] Train Loss: 0.426 | F1: 0.9727  |  Test F1: 0.9615 (Best: 0.9615)\n",
            "[22/50] Train Loss: 0.419 | F1: 0.9748  |  Test F1: 0.9639 (Best: 0.9639)\n",
            "[23/50] Train Loss: 0.414 | F1: 0.9757  |  Test F1: 0.9540 (Best: 0.9639)\n",
            "[24/50] Train Loss: 0.412 | F1: 0.9757  |  Test F1: 0.9610 (Best: 0.9639)\n",
            "[25/50] Train Loss: 0.404 | F1: 0.9799  |  Test F1: 0.9663 (Best: 0.9663)\n",
            "[26/50] Train Loss: 0.399 | F1: 0.9805  |  Test F1: 0.9619 (Best: 0.9663)\n",
            "[27/50] Train Loss: 0.397 | F1: 0.9813  |  Test F1: 0.9702 (Best: 0.9702)\n",
            "[28/50] Train Loss: 0.392 | F1: 0.9836  |  Test F1: 0.9701 (Best: 0.9702)\n",
            "[29/50] Train Loss: 0.392 | F1: 0.9842  |  Test F1: 0.9712 (Best: 0.9712)\n",
            "[30/50] Train Loss: 0.386 | F1: 0.9851  |  Test F1: 0.9719 (Best: 0.9719)\n",
            "[31/50] Train Loss: 0.379 | F1: 0.9872  |  Test F1: 0.9701 (Best: 0.9719)\n",
            "[32/50] Train Loss: 0.375 | F1: 0.9899  |  Test F1: 0.9695 (Best: 0.9719)\n",
            "[33/50] Train Loss: 0.374 | F1: 0.9896  |  Test F1: 0.9718 (Best: 0.9719)\n",
            "[34/50] Train Loss: 0.373 | F1: 0.9899  |  Test F1: 0.9741 (Best: 0.9741)\n",
            "[35/50] Train Loss: 0.368 | F1: 0.9912  |  Test F1: 0.9753 (Best: 0.9753)\n",
            "[36/50] Train Loss: 0.364 | F1: 0.9926  |  Test F1: 0.9727 (Best: 0.9753)\n",
            "[37/50] Train Loss: 0.363 | F1: 0.9939  |  Test F1: 0.9752 (Best: 0.9753)\n",
            "[38/50] Train Loss: 0.360 | F1: 0.9933  |  Test F1: 0.9771 (Best: 0.9771)\n",
            "[39/50] Train Loss: 0.359 | F1: 0.9940  |  Test F1: 0.9791 (Best: 0.9791)\n",
            "[40/50] Train Loss: 0.357 | F1: 0.9952  |  Test F1: 0.9787 (Best: 0.9791)\n",
            "[41/50] Train Loss: 0.355 | F1: 0.9954  |  Test F1: 0.9781 (Best: 0.9791)\n",
            "[42/50] Train Loss: 0.355 | F1: 0.9957  |  Test F1: 0.9775 (Best: 0.9791)\n",
            "[43/50] Train Loss: 0.354 | F1: 0.9958  |  Test F1: 0.9771 (Best: 0.9791)\n",
            "[44/50] Train Loss: 0.353 | F1: 0.9960  |  Test F1: 0.9764 (Best: 0.9791)\n",
            "[45/50] Train Loss: 0.352 | F1: 0.9969  |  Test F1: 0.9755 (Best: 0.9791)\n",
            "[46/50] Train Loss: 0.353 | F1: 0.9962  |  Test F1: 0.9771 (Best: 0.9791)\n",
            "[47/50] Train Loss: 0.351 | F1: 0.9965  |  Test F1: 0.9772 (Best: 0.9791)\n",
            "[48/50] Train Loss: 0.352 | F1: 0.9968  |  Test F1: 0.9761 (Best: 0.9791)\n",
            "[49/50] Train Loss: 0.351 | F1: 0.9968  |  Test F1: 0.9786 (Best: 0.9791)\n",
            "[50/50] Train Loss: 0.351 | F1: 0.9971  |  Test F1: 0.9780 (Best: 0.9791)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "EVALUATION...\n",
            "--------------------------------------------------------------------------------\n",
            "Final Result → Best Test F1: 0.9791 (Acc: 0.9811)\n",
            "\n",
            "================================================================================\n",
            " EXPERIMENT: AWGN Robustness\n",
            "================================================================================\n",
            "Loaded Best Model (F1: 0.9791) for testing...\n",
            "\n",
            "Sigma (c)  | Accuracy   | F1-Score  \n",
            "------------------------------------\n",
            "0.0        | 0.9811     | 0.9791    \n",
            "0.1        | 0.9473     | 0.9462    \n",
            "0.2        | 0.7755     | 0.7845    \n",
            "0.3        | 0.6047     | 0.5832    \n",
            "0.4        | 0.4871     | 0.4597    \n",
            "0.5        | 0.4139     | 0.3878    \n",
            "------------------------------------\n",
            "AWGN Noise Drift Experiment Completed.\n"
          ]
        }
      ]
    }
  ]
}