{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTO1H-ATNmZP",
        "outputId": "508769c7-0422-41b7-c852-65fb27fa8787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9), (7352,)\n",
            "(2947, 128, 9), (2947,)\n",
            "\n",
            "Total parameters: 28,998\n",
            "\n",
            "Starting Training (Standard CNN)...\n",
            "Epoch [10/50] Train F1: 0.9563 | Test F1: 0.9291 (Best: 0.9393)\n",
            "Epoch [20/50] Train F1: 0.9620 | Test F1: 0.9267 (Best: 0.9488)\n",
            "Epoch [30/50] Train F1: 0.9633 | Test F1: 0.9491 (Best: 0.9544)\n",
            "Epoch [40/50] Train F1: 0.9655 | Test F1: 0.9367 (Best: 0.9544)\n",
            "Epoch [50/50] Train F1: 0.9681 | Test F1: 0.9335 (Best: 0.9544)\n",
            "Inference Time   : 0.4153 ms / sample\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import contextlib\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "try:\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "    FVCORE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FVCORE_AVAILABLE = False\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. UCI-HAR Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_path, split='train'):\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        if split == 'train':\n",
        "            y = np.loadtxt(os.path.join(data_path, 'train', 'y_train.txt'))\n",
        "            signal_path = os.path.join(data_path, 'train', 'Inertial Signals')\n",
        "        else:\n",
        "            y = np.loadtxt(os.path.join(data_path, 'test', 'y_test.txt'))\n",
        "            signal_path = os.path.join(data_path, 'test', 'Inertial Signals')\n",
        "\n",
        "        signals = []\n",
        "        signal_files = [\n",
        "            'body_acc_x', 'body_acc_y', 'body_acc_z',\n",
        "            'body_gyro_x', 'body_gyro_y', 'body_gyro_z',\n",
        "            'total_acc_x', 'total_acc_y', 'total_acc_z'\n",
        "        ]\n",
        "\n",
        "        for signal_file in signal_files:\n",
        "            filename = os.path.join(signal_path, f'{signal_file}_{split}.txt')\n",
        "            signal_data = np.loadtxt(filename)\n",
        "            signals.append(signal_data)\n",
        "\n",
        "        self.X = np.stack(signals, axis=-1).astype(np.float32)\n",
        "        self.y = (y - 1).astype(np.int64)\n",
        "\n",
        "        print(f\"{self.X.shape}, {self.y.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.X[idx]), torch.LongTensor([self.y[idx]])[0]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. ASF Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64, num_classes=6, hidden_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. ASF-DCL과 동일한 Encoder\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "\n",
        "        # 2. Flow 모듈 없이 단순한 Classifier (Pooling 후 FC)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [Batch, Time, Dim]\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        # Global Average Pooling (시간 축 평균)\n",
        "        s_pool = torch.mean(s, dim=1)\n",
        "\n",
        "        logits = self.classifier(s_pool)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Train / Evaluation\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=0.05)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, f1\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "def measure_efficiency(model, input_shape=(1, 128, 9), warmup=10, iters=100):\n",
        "    \"\"\"\n",
        "    모델의 파라미터 수, FLOPs, 추론 속도를 측정합니다.\n",
        "    CPU 환경에서 측정하여 하드웨어 의존성을 줄인 벤치마크를 수행합니다.\n",
        "    \"\"\"\n",
        "    measure_device = torch.device('cpu')\n",
        "    model_cpu = copy.deepcopy(model).to(measure_device)\n",
        "    model_cpu.eval()\n",
        "\n",
        "    # 더미 입력 데이터 생성 (Batch=1)\n",
        "    real_input_shape = list(input_shape)\n",
        "    real_input_shape[0] = 1\n",
        "    sample_input = torch.randn(tuple(real_input_shape)).to(measure_device)\n",
        "\n",
        "    # 1) 파라미터 수\n",
        "    total_params = sum(p.numel() for p in model_cpu.parameters())\n",
        "    params_m = total_params / 1e6  # million params\n",
        "\n",
        "    # 2) FLOPs 측정 (fvcore 사용 가능할 때만)\n",
        "    flops_m = None\n",
        "    if FVCORE_AVAILABLE:\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                # 불필요한 출력 억제\n",
        "                fake_out = io.StringIO()\n",
        "                fake_err = io.StringIO()\n",
        "                with contextlib.redirect_stdout(fake_out), contextlib.redirect_stderr(fake_err):\n",
        "                    flops = FlopCountAnalysis(model_cpu, (sample_input,))\n",
        "                    total_flops = flops.total()\n",
        "                flops_m = total_flops / 1e6  # to millions\n",
        "        except Exception as e:\n",
        "            print(f\"FLOPs calculation failed: {e}\")\n",
        "            flops_m = None\n",
        "\n",
        "    # 3) 추론 시간 측정\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(warmup):\n",
        "            _ = model_cpu(sample_input)\n",
        "\n",
        "        start = time.time()\n",
        "        for _ in range(iters):\n",
        "            _ = model_cpu(sample_input)\n",
        "        end = time.time()\n",
        "\n",
        "    avg_sec = (end - start) / iters\n",
        "    inference_ms = avg_sec * 1000.0\n",
        "\n",
        "    del model_cpu\n",
        "\n",
        "    return {\n",
        "        \"params_m\": params_m,\n",
        "        \"flops_m\": flops_m,\n",
        "        \"inference_ms\": inference_ms,\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Efficiency Measurement Function\n",
        "# ------------------------------------------------------------------------------\n",
        "def measure_efficiency(model, input_shape=(1, 128, 9), warmup=10, iters=100):\n",
        "    \"\"\"\n",
        "    모델의 파라미터 수, FLOPs, 추론 속도를 측정합니다.\n",
        "    CPU 환경에서 측정하여 하드웨어 의존성을 줄인 벤치마크를 수행합니다.\n",
        "    \"\"\"\n",
        "    measure_device = torch.device('cpu')\n",
        "    model_cpu = copy.deepcopy(model).to(measure_device)\n",
        "    model_cpu.eval()\n",
        "\n",
        "    # 더미 입력 데이터 생성 (Batch=1)\n",
        "    real_input_shape = list(input_shape)\n",
        "    real_input_shape[0] = 1\n",
        "    sample_input = torch.randn(tuple(real_input_shape)).to(measure_device)\n",
        "\n",
        "    # 1) 파라미터 수\n",
        "    total_params = sum(p.numel() for p in model_cpu.parameters())\n",
        "    params_m = total_params / 1e6  # million params\n",
        "\n",
        "    # 2) FLOPs 측정 (fvcore 사용 가능할 때만)\n",
        "    flops_m = None\n",
        "    if FVCORE_AVAILABLE:\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                # 불필요한 출력 억제\n",
        "                fake_out = io.StringIO()\n",
        "                fake_err = io.StringIO()\n",
        "                with contextlib.redirect_stdout(fake_out), contextlib.redirect_stderr(fake_err):\n",
        "                    flops = FlopCountAnalysis(model_cpu, (sample_input,))\n",
        "                    total_flops = flops.total()\n",
        "                flops_m = total_flops / 1e6  # to millions\n",
        "        except Exception as e:\n",
        "            print(f\"FLOPs calculation failed: {e}\")\n",
        "            flops_m = None\n",
        "\n",
        "    # 3) 추론 시간 측정\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(warmup):\n",
        "            _ = model_cpu(sample_input)\n",
        "\n",
        "        start = time.time()\n",
        "        for _ in range(iters):\n",
        "            _ = model_cpu(sample_input)\n",
        "        end = time.time()\n",
        "\n",
        "    avg_sec = (end - start) / iters\n",
        "    inference_ms = avg_sec * 1000.0\n",
        "\n",
        "    del model_cpu\n",
        "\n",
        "    return {\n",
        "        \"params_m\": params_m,\n",
        "        \"flops_m\": flops_m,\n",
        "        \"inference_ms\": inference_ms,\n",
        "    }\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Main Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data/UCI_HAR'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    train_dataset = UCIHARDataset(DATA_PATH, split='train')\n",
        "    test_dataset = UCIHARDataset(DATA_PATH, split='test')\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, num_workers=2,\n",
        "                              worker_init_fn=seed_worker,\n",
        "                              generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, num_workers=2,\n",
        "                             worker_init_fn=seed_worker,\n",
        "                             generator=g)\n",
        "\n",
        "    model = StandardCNN(\n",
        "        input_channels=9,\n",
        "        latent_dim=64,\n",
        "        num_classes=6,\n",
        "        hidden_dim=64\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print()\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=LEARNING_RATE,\n",
        "                                 weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    print(\"\\nStarting Training (Standard CNN)...\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t_loss, t_f1 = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "\n",
        "        v_acc, v_f1 = evaluate(model, test_loader, DEVICE)\n",
        "\n",
        "        if v_f1 > best_f1:\n",
        "            best_f1 = v_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train F1: {t_f1:.4f} | Test F1: {v_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "\n",
        "    metrics = measure_efficiency(model, input_shape=(1, 128, 9), warmup=10, iters=100)\n",
        "    print(f\"Inference Time   : {metrics['inference_ms']:.4f} ms / sample\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}