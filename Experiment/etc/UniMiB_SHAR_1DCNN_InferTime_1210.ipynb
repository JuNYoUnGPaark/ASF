{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTO1H-ATNmZP",
        "outputId": "a36c8a0c-e817-45df-ec64-579d40f20d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN] Loaded UniMiB-SHAR ADL: 6063 samples (Shape: (151, 3))\n",
            "[TEST] Loaded UniMiB-SHAR ADL: 1516 samples (Shape: (151, 3))\n",
            "\n",
            "Total parameters: 28,233\n",
            "\n",
            "Starting Training (Standard CNN)...\n",
            "Epoch [10/50] Train F1: 0.7120 | Test F1: 0.7559 (Best: 0.7791)\n",
            "Epoch [20/50] Train F1: 0.8026 | Test F1: 0.8660 (Best: 0.8751)\n",
            "Epoch [30/50] Train F1: 0.8378 | Test F1: 0.9075 (Best: 0.9075)\n",
            "Epoch [40/50] Train F1: 0.8901 | Test F1: 0.9150 (Best: 0.9150)\n",
            "Epoch [50/50] Train F1: 0.9083 | Test F1: 0.9416 (Best: 0.9416)\n",
            "Inference Time   : 0.4233 ms / sample\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "import io\n",
        "import contextlib\n",
        "\n",
        "try:\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "    FVCORE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FVCORE_AVAILABLE = False\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. UniMiBARDataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class UniMiBARDataset(Dataset):\n",
        "    def __init__(self, data_path, split='train', train_ratio=0.8, seed=42):\n",
        "        self.split = split\n",
        "\n",
        "        # 1. 파일 로드 (파일명이 대소문자 구분이 있을 수 있으니 확인 필요)\n",
        "        # 예: adl_data.npy, adl_labels.npy\n",
        "        x_path = os.path.join(data_path, 'adl_data.npy')\n",
        "        y_path = os.path.join(data_path, 'adl_labels.npy')\n",
        "\n",
        "        # 데이터가 없으면 에러 발생\n",
        "        if not os.path.exists(x_path):\n",
        "            raise FileNotFoundError(f\"File not found: {x_path}\")\n",
        "\n",
        "        raw_x = np.load(x_path)\n",
        "        raw_y = np.load(y_path)\n",
        "\n",
        "        # 2. 데이터 전처리\n",
        "        # (N, 453) -> (N, 3, 151) -> (N, 151, 3)\n",
        "        # UniMiB는 보통 [x...x, y...y, z...z] 순서\n",
        "        if raw_x.ndim == 2:\n",
        "            raw_x = raw_x.reshape(-1, 3, 151).transpose(0, 2, 1)\n",
        "\n",
        "        # 라벨 처리: (N, 3) -> 첫번째 컬럼(Action ID)만 사용\n",
        "        # 클래스가 1~9 이므로 0~8로 변환하기 위해 -1\n",
        "        if raw_y.ndim > 1:\n",
        "            raw_y = raw_y[:, 0]\n",
        "\n",
        "        self.y_all = (raw_y - 1).astype(np.int64)\n",
        "        self.X_all = raw_x.astype(np.float32)\n",
        "\n",
        "        # 3. Train / Test 분할 (고정 시드 사용)\n",
        "        total_len = len(self.X_all)\n",
        "        indices = np.arange(total_len)\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        split_idx = int(total_len * train_ratio)\n",
        "\n",
        "        if split == 'train':\n",
        "            self.indices = indices[:split_idx]\n",
        "        else:\n",
        "            self.indices = indices[split_idx:]\n",
        "\n",
        "        print(f\"[{split.upper()}] Loaded UniMiB-SHAR ADL: {len(self.indices)} samples (Shape: {self.X_all.shape[1:]})\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]\n",
        "        return torch.FloatTensor(self.X_all[real_idx]), torch.LongTensor([self.y_all[real_idx]])[0]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. ASF Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64, num_classes=6, hidden_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. ASF-DCL과 동일한 Encoder\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "\n",
        "        # 2. Flow 모듈 없이 단순한 Classifier (Pooling 후 FC)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [Batch, Time, Dim]\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        # Global Average Pooling (시간 축 평균)\n",
        "        s_pool = torch.mean(s, dim=1)\n",
        "\n",
        "        logits = self.classifier(s_pool)\n",
        "        return logits\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Train / Evaluation\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=0.05)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, f1\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "def measure_efficiency(model, input_shape=(1, 128, 9), warmup=10, iters=100):\n",
        "    \"\"\"\n",
        "    모델의 파라미터 수, FLOPs, 추론 속도를 측정합니다.\n",
        "    CPU 환경에서 측정하여 하드웨어 의존성을 줄인 벤치마크를 수행합니다.\n",
        "    \"\"\"\n",
        "    measure_device = torch.device('cpu')\n",
        "    model_cpu = copy.deepcopy(model).to(measure_device)\n",
        "    model_cpu.eval()\n",
        "\n",
        "    # 더미 입력 데이터 생성 (Batch=1)\n",
        "    real_input_shape = list(input_shape)\n",
        "    real_input_shape[0] = 1\n",
        "    sample_input = torch.randn(tuple(real_input_shape)).to(measure_device)\n",
        "\n",
        "    # 1) 파라미터 수\n",
        "    total_params = sum(p.numel() for p in model_cpu.parameters())\n",
        "    params_m = total_params / 1e6  # million params\n",
        "\n",
        "    # 2) FLOPs 측정 (fvcore 사용 가능할 때만)\n",
        "    flops_m = None\n",
        "    if FVCORE_AVAILABLE:\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                # 불필요한 출력 억제\n",
        "                fake_out = io.StringIO()\n",
        "                fake_err = io.StringIO()\n",
        "                with contextlib.redirect_stdout(fake_out), contextlib.redirect_stderr(fake_err):\n",
        "                    flops = FlopCountAnalysis(model_cpu, (sample_input,))\n",
        "                    total_flops = flops.total()\n",
        "                flops_m = total_flops / 1e6  # to millions\n",
        "        except Exception as e:\n",
        "            print(f\"FLOPs calculation failed: {e}\")\n",
        "            flops_m = None\n",
        "\n",
        "    # 3) 추론 시간 측정\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(warmup):\n",
        "            _ = model_cpu(sample_input)\n",
        "\n",
        "        start = time.time()\n",
        "        for _ in range(iters):\n",
        "            _ = model_cpu(sample_input)\n",
        "        end = time.time()\n",
        "\n",
        "    avg_sec = (end - start) / iters\n",
        "    inference_ms = avg_sec * 1000.0\n",
        "\n",
        "    del model_cpu\n",
        "\n",
        "    return {\n",
        "        \"params_m\": params_m,\n",
        "        \"flops_m\": flops_m,\n",
        "        \"inference_ms\": inference_ms,\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Main Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data/UniMiB-SHAR'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    train_dataset = UniMiBARDataset(DATA_PATH, split='train', train_ratio=0.8, seed=SEED)\n",
        "    test_dataset = UniMiBARDataset(DATA_PATH, split='test', train_ratio=0.8, seed=SEED)\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, num_workers=2,\n",
        "                              worker_init_fn=seed_worker,\n",
        "                              generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, num_workers=2,\n",
        "                             worker_init_fn=seed_worker,\n",
        "                             generator=g)\n",
        "\n",
        "    model = StandardCNN(\n",
        "        input_channels=3,\n",
        "        latent_dim=64,\n",
        "        num_classes=9,\n",
        "        hidden_dim=64\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print()\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=LEARNING_RATE,\n",
        "                                 weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    print(\"\\nStarting Training (Standard CNN)...\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t_loss, t_f1 = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "\n",
        "        v_acc, v_f1 = evaluate(model, test_loader, DEVICE)\n",
        "\n",
        "        if v_f1 > best_f1:\n",
        "            best_f1 = v_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train F1: {t_f1:.4f} | Test F1: {v_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "\n",
        "    metrics = measure_efficiency(model, input_shape=(1, 151, 3), warmup=10, iters=100)\n",
        "    print(f\"Inference Time   : {metrics['inference_ms']:.4f} ms / sample\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}