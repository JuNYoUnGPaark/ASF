{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTO1H-ATNmZP",
        "outputId": "16ccd58a-e746-431e-bebc-06c905b0e9b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Standard 1D-CNN Baseline on cuda\n",
            "================================================================================\n",
            "Loaded WISDM dataset (single txt)\n",
            "  X shape       : (27108, 80, 3)  (N, T, C)\n",
            "  y shape       : (27108,)  (N,)\n",
            "  subjects shape: (27108,) (N,)\n",
            "  unique subjects: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34), np.int64(35), np.int64(36)]\n",
            "================================================================================\n",
            "\n",
            "Starting Training (Standard CNN)...\n",
            "Epoch [10/50] Train F1: 0.9619 | Test F1: 0.9625 (Best: 0.9625)\n",
            "Epoch [20/50] Train F1: 0.9720 | Test F1: 0.9658 (Best: 0.9690)\n",
            "Epoch [30/50] Train F1: 0.9750 | Test F1: 0.9687 (Best: 0.9707)\n",
            "Epoch [40/50] Train F1: 0.9794 | Test F1: 0.9552 (Best: 0.9730)\n",
            "Epoch [50/50] Train F1: 0.9838 | Test F1: 0.9695 (Best: 0.9741)\n",
            "Inference Time   : 0.3732 ms / sample\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import io\n",
        "import contextlib\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "try:\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "    FVCORE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FVCORE_AVAILABLE = False\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 0. Utils & Setup\n",
        "# ------------------------------------------------------------------------------\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. WISDMDataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class WISDMDataset(Dataset):\n",
        "    \"\"\"\n",
        "    단일 WISDM txt 형식:\n",
        "    subject,activity,timestamp,x,y,z;\n",
        "    예) 33,Jogging,49105962326000,-0.6946377,12.680544,0.50395286;\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path: str, window_size: int = 80, step_size: int = 40):\n",
        "        super().__init__()\n",
        "        self.file_path = file_path\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "\n",
        "        if not os.path.isfile(file_path):\n",
        "            raise FileNotFoundError(f\"WISDM txt file not found: {file_path}\")\n",
        "\n",
        "        df = self._load_file(file_path)\n",
        "        self.X, self.y, self.subjects = self._create_windows(df)\n",
        "\n",
        "        self.unique_subjects = sorted(np.unique(self.subjects))\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Loaded WISDM dataset (single txt)\")\n",
        "        print(f\"  X shape       : {self.X.shape}  (N, T, C)\")\n",
        "        print(f\"  y shape       : {self.y.shape}  (N,)\")\n",
        "        print(f\"  subjects shape: {self.subjects.shape} (N,)\")\n",
        "        print(f\"  unique subjects: {self.unique_subjects}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def _load_file(self, file_path: str) -> pd.DataFrame:\n",
        "        \"\"\"원본 txt 한 개를 통째로 읽어서 DataFrame으로 변환.\"\"\"\n",
        "        with open(file_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        rows = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            # 끝 세미콜론 제거\n",
        "            line = line.replace(\";\", \"\")\n",
        "            parts = line.split(\",\")\n",
        "\n",
        "            # subject, activity, timestamp, x, y, z → 6개 아니면 스킵\n",
        "            if len(parts) != 6:\n",
        "                continue\n",
        "\n",
        "            subj, act, ts, x, y, z = parts\n",
        "\n",
        "            # x,y,z 중 하나라도 비어있으면 스킵\n",
        "            if x.strip() == \"\" or y.strip() == \"\" or z.strip() == \"\":\n",
        "                continue\n",
        "\n",
        "            rows.append([subj, act, ts, x, y, z])\n",
        "\n",
        "        if not rows:\n",
        "            raise ValueError(f\"No valid rows parsed from file: {file_path}\")\n",
        "\n",
        "        df = pd.DataFrame(rows, columns=[\"subject\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        # 문자열 → NaN 처리 후 숫자로 변환\n",
        "        df = df.replace([\"\", \"NaN\", \"nan\"], np.nan).dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        df[\"subject\"] = pd.to_numeric(df[\"subject\"], errors=\"coerce\")\n",
        "        df[\"x\"] = pd.to_numeric(df[\"x\"], errors=\"coerce\")\n",
        "        df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\")\n",
        "        df[\"z\"] = pd.to_numeric(df[\"z\"], errors=\"coerce\")\n",
        "\n",
        "        df = df.dropna(subset=[\"subject\", \"x\", \"y\", \"z\"])\n",
        "\n",
        "        if df.empty:\n",
        "            raise ValueError(\"After cleaning, WISDM DataFrame is empty. Check file format.\")\n",
        "\n",
        "        df[\"subject\"] = df[\"subject\"].astype(int)\n",
        "\n",
        "        # activity 문자열 → 정수 라벨\n",
        "        df[\"activity_id\"] = df[\"activity\"].astype(\"category\").cat.codes\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _create_windows(self, df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        subject 별로 나눠서 sliding window 생성.\n",
        "        X: (N, T, 3), y: (N,), subjects: (N,)\n",
        "        \"\"\"\n",
        "        X_list, y_list, s_list = [], [], []\n",
        "\n",
        "        for subj_id in sorted(df[\"subject\"].unique()):\n",
        "            df_sub = df[df[\"subject\"] == subj_id]\n",
        "\n",
        "            # 필요하면 여기서 activity별로도 끊을 수 있음 (원하면 확장 가능)\n",
        "            data = df_sub[[\"x\", \"y\", \"z\"]].to_numpy(dtype=np.float32)      # (L, 3)\n",
        "            labels = df_sub[\"activity_id\"].to_numpy(dtype=np.int64)        # (L,)\n",
        "            L = len(df_sub)\n",
        "\n",
        "            start = 0\n",
        "            while start + self.window_size <= L:\n",
        "                end = start + self.window_size\n",
        "\n",
        "                window_x = data[start:end]          # (T, 3)\n",
        "                window_y = labels[end - 1]          # 마지막 타임스텝 라벨\n",
        "\n",
        "                X_list.append(window_x.T)           # (3, T)\n",
        "                y_list.append(window_y)\n",
        "                s_list.append(subj_id)\n",
        "\n",
        "                start += self.step_size\n",
        "\n",
        "        if len(X_list) == 0:\n",
        "            raise ValueError(\"[WISDMDataset] No windows created. Try smaller window_size or check data.\")\n",
        "\n",
        "        X = np.stack(X_list, axis=0).astype(np.float32)  # (N, 3, T)\n",
        "        y = np.array(y_list, dtype=np.int64)\n",
        "        s = np.array(s_list, dtype=np.int64)\n",
        "\n",
        "        # (N, 3, T) → (N, T, 3)\n",
        "        X = X.transpose(0, 2, 1)\n",
        "        return X, y, s\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return (\n",
        "            torch.FloatTensor(self.X[idx]),          # (T, 3)\n",
        "            torch.LongTensor([self.y[idx]])[0],\n",
        "            self.subjects[idx],\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. Baseline Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "# ASF-DCL과 공정한 비교를 위해 동일한 Encoder 구조 사용\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "# Baseline Model: Encoder + Global Average Pooling + Classifier\n",
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64, num_classes=6, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "\n",
        "        # Flow 모듈 없이 바로 분류 (일반적인 CNN 구조)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Encode: [Batch, Time, Dim]\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        # 2. Global Average Pooling (Time 축 평균)\n",
        "        s_pool = torch.mean(s, dim=1)\n",
        "\n",
        "        # 3. Classify\n",
        "        logits = self.classifier(s_pool)\n",
        "        return logits\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. Train & Evaluate Functions (Baseline용)\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=0.05)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, f1\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "def measure_efficiency(model, input_shape=(1, 128, 9), warmup=10, iters=100):\n",
        "    \"\"\"\n",
        "    모델의 파라미터 수, FLOPs, 추론 속도를 측정합니다.\n",
        "    CPU 환경에서 측정하여 하드웨어 의존성을 줄인 벤치마크를 수행합니다.\n",
        "    \"\"\"\n",
        "    measure_device = torch.device('cpu')\n",
        "    model_cpu = copy.deepcopy(model).to(measure_device)\n",
        "    model_cpu.eval()\n",
        "\n",
        "    # 더미 입력 데이터 생성 (Batch=1)\n",
        "    real_input_shape = list(input_shape)\n",
        "    real_input_shape[0] = 1\n",
        "    sample_input = torch.randn(tuple(real_input_shape)).to(measure_device)\n",
        "\n",
        "    # 1) 파라미터 수\n",
        "    total_params = sum(p.numel() for p in model_cpu.parameters())\n",
        "    params_m = total_params / 1e6  # million params\n",
        "\n",
        "    # 2) FLOPs 측정 (fvcore 사용 가능할 때만)\n",
        "    flops_m = None\n",
        "    if FVCORE_AVAILABLE:\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                # 불필요한 출력 억제\n",
        "                fake_out = io.StringIO()\n",
        "                fake_err = io.StringIO()\n",
        "                with contextlib.redirect_stdout(fake_out), contextlib.redirect_stderr(fake_err):\n",
        "                    flops = FlopCountAnalysis(model_cpu, (sample_input,))\n",
        "                    total_flops = flops.total()\n",
        "                flops_m = total_flops / 1e6  # to millions\n",
        "        except Exception as e:\n",
        "            print(f\"FLOPs calculation failed: {e}\")\n",
        "            flops_m = None\n",
        "\n",
        "    # 3) 추론 시간 측정\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(warmup):\n",
        "            _ = model_cpu(sample_input)\n",
        "\n",
        "        start = time.time()\n",
        "        for _ in range(iters):\n",
        "            _ = model_cpu(sample_input)\n",
        "        end = time.time()\n",
        "\n",
        "    avg_sec = (end - start) / iters\n",
        "    inference_ms = avg_sec * 1000.0\n",
        "\n",
        "    del model_cpu\n",
        "\n",
        "    return {\n",
        "        \"params_m\": params_m,\n",
        "        \"flops_m\": flops_m,\n",
        "        \"inference_ms\": inference_ms,\n",
        "    }\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. Main Execution\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    # 설정 (기존과 동일하게 맞춤)\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data/WISDM_ar_v1.1_raw.txt'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Running Standard 1D-CNN Baseline on {DEVICE}\")\n",
        "\n",
        "    # 데이터 로드\n",
        "    full_dataset = WISDMDataset(DATA_PATH, window_size=80, step_size=40)\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(total_size * 0.8)\n",
        "    test_size = total_size - train_size\n",
        "\n",
        "    train_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, test_size],\n",
        "        generator=torch.Generator().manual_seed(SEED)\n",
        "    )\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              worker_init_fn=seed_worker, generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                             worker_init_fn=seed_worker, generator=g)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = StandardCNN(input_channels=3, latent_dim=64, num_classes=6).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    print(\"\\nStarting Training (Standard CNN)...\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t_loss, t_f1 = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "\n",
        "        v_acc, v_f1 = evaluate(model, test_loader, DEVICE)\n",
        "\n",
        "        if v_f1 > best_f1:\n",
        "            best_f1 = v_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train F1: {t_f1:.4f} | Test F1: {v_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "\n",
        "    metrics = measure_efficiency(model, input_shape=(1, 128, 3), warmup=10, iters=100)\n",
        "    print(f\"Inference Time   : {metrics['inference_ms']:.4f} ms / sample\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}