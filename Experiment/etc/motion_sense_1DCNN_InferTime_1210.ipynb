{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTO1H-ATNmZP",
        "outputId": "cda7b53b-f866-453d-c402-8fd8115bf66f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MotionSenseDataset] windows: 22053, classes: 6\n",
            "Classes map: {'dws': 0, 'jog': 1, 'sit': 2, 'std': 3, 'ups': 4, 'wlk': 5}\n",
            "Total samples: 22053 | Train: 17642 | Test: 4411\n",
            "\n",
            "Total parameters: 28,518\n",
            "\n",
            "Starting Training (Standard CNN)...\n",
            "Epoch [10/50] Train F1: 0.9858 | Test F1: 0.9868 (Best: 0.9879)\n",
            "Epoch [20/50] Train F1: 0.9948 | Test F1: 0.9857 (Best: 0.9894)\n",
            "Epoch [30/50] Train F1: 0.9974 | Test F1: 0.9914 (Best: 0.9950)\n",
            "Epoch [40/50] Train F1: 0.9975 | Test F1: 0.9925 (Best: 0.9967)\n",
            "Epoch [50/50] Train F1: 0.9986 | Test F1: 0.9909 (Best: 0.9967)\n",
            "Inference Time   : 0.3804 ms / sample\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import contextlib\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "try:\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "    FVCORE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FVCORE_AVAILABLE = False\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. motion-sense Dataset\n",
        "# ------------------------------------------------------------------------------\n",
        "class MotionSenseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Motion-Sense Dataset Loader\n",
        "    - 입력: (T, 6) -> (userAcceleration.x, y, z, rotationRate.x, y, z)\n",
        "    - 라벨: 폴더명(dws, ups, wlk, jog, sit, std)을 파싱하여 인덱스로 매핑\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root_dir,\n",
        "        window_size=128,\n",
        "        step_size=64,\n",
        "        normalize=True,\n",
        "        target_subjects=None,\n",
        "        scaler=None\n",
        "    ):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "        self.normalize = normalize\n",
        "\n",
        "        # Motion-Sense의 데이터 폴더 경로 (이미지 기준 A_DeviceMotion_data 폴더)\n",
        "        self.data_dir = self.root_dir / \"A_DeviceMotion_data\"\n",
        "\n",
        "        # 1) 데이터 로드 및 통합\n",
        "        df_all = self._load_all_data()\n",
        "\n",
        "        if target_subjects is not None:\n",
        "            df_all = df_all[df_all['subject_id'].isin(target_subjects)].copy()\n",
        "            print(f\"Dataset initialized with subjects: {target_subjects}\")\n",
        "            print(f\"Total rows after filtering: {len(df_all)}\")\n",
        "\n",
        "        # 2) 라벨 -> 인덱스 매핑\n",
        "        # Motion-Sense의 6개 클래스: dws, ups, wlk, jog, sit, std\n",
        "        activities = ['dws', 'jog', 'sit', 'std', 'ups', 'wlk']\n",
        "        self.label2idx = {label: i for i, label in enumerate(activities)}\n",
        "        self.idx2label = {i: label for label, i in self.label2idx.items()}\n",
        "        df_all[\"label_idx\"] = df_all[\"activity\"].map(self.label2idx)\n",
        "\n",
        "        # 3) 정규화 (StandardScaler)\n",
        "        # MotionSense 컬럼: userAcceleration.x/y/z (acc), rotationRate.x/y/z (gyro)\n",
        "        feat_cols = [\n",
        "            \"userAcceleration.x\", \"userAcceleration.y\", \"userAcceleration.z\",\n",
        "            \"rotationRate.x\", \"rotationRate.y\", \"rotationRate.z\"\n",
        "        ]\n",
        "        feats = df_all[feat_cols].values.astype(np.float32)\n",
        "\n",
        "        if self.normalize:\n",
        "            if scaler is None:\n",
        "                # 스케일러가 없으면(Train용) -> 새로 맞춤(fit)\n",
        "                self.scaler = StandardScaler()\n",
        "                feats = self.scaler.fit_transform(feats)\n",
        "            else:\n",
        "                # 스케일러가 있으면(Test용) -> 기존 것 사용(transform)\n",
        "                self.scaler = scaler\n",
        "                feats = self.scaler.transform(feats)\n",
        "        else:\n",
        "            self.scaler = None\n",
        "\n",
        "        df_all[feat_cols] = feats\n",
        "\n",
        "        # 4) 슬라이딩 윈도우 생성 (Subject, Activity, Trial 별로 그룹화)\n",
        "        X_list = []\n",
        "        y_list = []\n",
        "\n",
        "        # trial_id는 각 csv 파일을 구분하기 위해 _load_all_data에서 생성해야 함\n",
        "        for _, g in df_all.groupby([\"subject_id\", \"activity\", \"trial_id\"]):\n",
        "            g = g.sort_values(\"timestamp_idx\").reset_index(drop=True)\n",
        "\n",
        "            data = g[feat_cols].values\n",
        "            labels = g[\"label_idx\"].values\n",
        "            n = len(g)\n",
        "\n",
        "            if n < window_size:\n",
        "                continue\n",
        "\n",
        "            for start in range(0, n - window_size + 1, step_size):\n",
        "                end = start + window_size\n",
        "                w_data = data[start:end]\n",
        "                w_labels = labels[start:end]\n",
        "\n",
        "                # 윈도우 라벨 (Mode)\n",
        "                majority_label = np.bincount(w_labels).argmax()\n",
        "\n",
        "                X_list.append(w_data.astype(np.float32))\n",
        "                y_list.append(majority_label)\n",
        "\n",
        "        self.X = np.stack(X_list) if len(X_list) > 0 else np.zeros((0, window_size, 6), dtype=np.float32)\n",
        "        self.y = np.array(y_list, dtype=np.int64)\n",
        "\n",
        "        print(f\"[MotionSenseDataset] windows: {len(self.X)}, classes: {len(self.label2idx)}\")\n",
        "        print(f\"Classes map: {self.label2idx}\")\n",
        "\n",
        "    def _load_all_data(self):\n",
        "        \"\"\"\n",
        "        A_DeviceMotion_data 내부의 모든 폴더를 순회하며 CSV 로드\n",
        "        \"\"\"\n",
        "        all_dfs = []\n",
        "\n",
        "        # data_dir 내부의 폴더들 (예: dws_1, jog_9 ...)\n",
        "        if not self.data_dir.exists():\n",
        "             raise FileNotFoundError(f\"Directory not found: {self.data_dir}\")\n",
        "\n",
        "        for folder in os.listdir(self.data_dir):\n",
        "            folder_path = self.data_dir / folder\n",
        "            if not folder_path.is_dir():\n",
        "                continue\n",
        "\n",
        "            # 폴더명 파싱 (예: dws_1 -> activity=dws, subject=1)\n",
        "            parts = folder.split('_')\n",
        "            activity_label = parts[0]\n",
        "            subject_id = parts[1]\n",
        "\n",
        "            # 폴더 내 csv 파일 읽기 (보통 sub_1.csv 같은 형태)\n",
        "            for csv_file in os.listdir(folder_path):\n",
        "                if not csv_file.endswith(\".csv\"):\n",
        "                    continue\n",
        "\n",
        "                file_path = folder_path / csv_file\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                # Unnamed: 0 컬럼이 타임스탬프 역할(인덱스)\n",
        "                if \"Unnamed: 0\" in df.columns:\n",
        "                    df = df.rename(columns={\"Unnamed: 0\": \"timestamp_idx\"})\n",
        "                else:\n",
        "                    df[\"timestamp_idx\"] = range(len(df))\n",
        "\n",
        "                df[\"activity\"] = activity_label\n",
        "                df[\"subject_id\"] = int(subject_id)\n",
        "                df[\"trial_id\"] = folder  # 폴더명 자체를 trial 식별자로 사용\n",
        "\n",
        "                all_dfs.append(df)\n",
        "\n",
        "        return pd.concat(all_dfs, ignore_index=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. ASF Model Components\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class LatentEncoder(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.conv3 = nn.Conv1d(64, latent_dim, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        h = F.relu(self.bn1(self.conv1(x)))\n",
        "        h = F.relu(self.bn2(self.conv2(h)))\n",
        "        s = F.relu(self.bn3(self.conv3(h)))\n",
        "        s = s.transpose(1, 2)\n",
        "        return s\n",
        "\n",
        "class StandardCNN(nn.Module):\n",
        "    def __init__(self, input_channels=9, latent_dim=64, num_classes=6, hidden_dim=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. ASF-DCL과 동일한 Encoder\n",
        "        self.latent_encoder = LatentEncoder(input_channels, latent_dim)\n",
        "\n",
        "        # 2. Flow 모듈 없이 단순한 Classifier (Pooling 후 FC)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # [Batch, Time, Dim]\n",
        "        s = self.latent_encoder(x)\n",
        "\n",
        "        # Global Average Pooling (시간 축 평균)\n",
        "        s_pool = torch.mean(s, dim=1)\n",
        "\n",
        "        logits = self.classifier(s_pool)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5. Train / Evaluation\n",
        "# ------------------------------------------------------------------------------\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y, label_smoothing=0.05)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        all_preds.extend(preds.detach().cpu().numpy())\n",
        "        all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return avg_loss, f1\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    AWGN 노이즈를 주입하여 모델의 견고성을 평가하는 함수\n",
        "    sigma: 노이즈 강도 (Standard Deviation)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x = batch[0].to(device)\n",
        "            y = batch[1].to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_preds.extend(preds.detach().cpu().numpy())\n",
        "            all_labels.extend(y.detach().cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "def measure_efficiency(model, input_shape=(1, 128, 9), warmup=10, iters=100):\n",
        "    \"\"\"\n",
        "    모델의 파라미터 수, FLOPs, 추론 속도를 측정합니다.\n",
        "    CPU 환경에서 측정하여 하드웨어 의존성을 줄인 벤치마크를 수행합니다.\n",
        "    \"\"\"\n",
        "    measure_device = torch.device('cpu')\n",
        "    model_cpu = copy.deepcopy(model).to(measure_device)\n",
        "    model_cpu.eval()\n",
        "\n",
        "    # 더미 입력 데이터 생성 (Batch=1)\n",
        "    real_input_shape = list(input_shape)\n",
        "    real_input_shape[0] = 1\n",
        "    sample_input = torch.randn(tuple(real_input_shape)).to(measure_device)\n",
        "\n",
        "    # 1) 파라미터 수\n",
        "    total_params = sum(p.numel() for p in model_cpu.parameters())\n",
        "    params_m = total_params / 1e6  # million params\n",
        "\n",
        "    # 2) FLOPs 측정 (fvcore 사용 가능할 때만)\n",
        "    flops_m = None\n",
        "    if FVCORE_AVAILABLE:\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                # 불필요한 출력 억제\n",
        "                fake_out = io.StringIO()\n",
        "                fake_err = io.StringIO()\n",
        "                with contextlib.redirect_stdout(fake_out), contextlib.redirect_stderr(fake_err):\n",
        "                    flops = FlopCountAnalysis(model_cpu, (sample_input,))\n",
        "                    total_flops = flops.total()\n",
        "                flops_m = total_flops / 1e6  # to millions\n",
        "        except Exception as e:\n",
        "            print(f\"FLOPs calculation failed: {e}\")\n",
        "            flops_m = None\n",
        "\n",
        "    # 3) 추론 시간 측정\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(warmup):\n",
        "            _ = model_cpu(sample_input)\n",
        "\n",
        "        start = time.time()\n",
        "        for _ in range(iters):\n",
        "            _ = model_cpu(sample_input)\n",
        "        end = time.time()\n",
        "\n",
        "    avg_sec = (end - start) / iters\n",
        "    inference_ms = avg_sec * 1000.0\n",
        "\n",
        "    del model_cpu\n",
        "\n",
        "    return {\n",
        "        \"params_m\": params_m,\n",
        "        \"flops_m\": flops_m,\n",
        "        \"inference_ms\": inference_ms,\n",
        "    }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6. Main Training Loop\n",
        "# ------------------------------------------------------------------------------\n",
        "def main():\n",
        "    SEED = 42\n",
        "    set_seed(SEED)\n",
        "\n",
        "    DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/HAR_data/motion-sense'\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    full_dataset = MotionSenseDataset(\n",
        "        root_dir=DATA_PATH,\n",
        "        target_subjects=None,\n",
        "        normalize=True\n",
        "    )\n",
        "\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(total_size * 0.8)\n",
        "    test_size = total_size - train_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(SEED)\n",
        "    train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size], generator=generator)\n",
        "    print(f\"Total samples: {total_size} | Train: {len(train_dataset)} | Test: {len(test_dataset)}\")\n",
        "\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(SEED)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, num_workers=2,\n",
        "                              worker_init_fn=seed_worker,\n",
        "                              generator=g)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False, num_workers=2,\n",
        "                             worker_init_fn=seed_worker,\n",
        "                             generator=g)\n",
        "\n",
        "    model = StandardCNN(\n",
        "        input_channels=6,\n",
        "        latent_dim=64,\n",
        "        num_classes=6,\n",
        "        hidden_dim=64\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print()\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=LEARNING_RATE,\n",
        "                                 weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=NUM_EPOCHS\n",
        "    )\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # --- 학습 루프 ---\n",
        "    print(\"\\nStarting Training (Standard CNN)...\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t_loss, t_f1 = train_epoch(model, train_loader, optimizer, DEVICE)\n",
        "\n",
        "        v_acc, v_f1 = evaluate(model, test_loader, DEVICE)\n",
        "\n",
        "        if v_f1 > best_f1:\n",
        "            best_f1 = v_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Train F1: {t_f1:.4f} | Test F1: {v_f1:.4f} (Best: {best_f1:.4f})\")\n",
        "\n",
        "    metrics = measure_efficiency(model, input_shape=(1, 128, 6), warmup=10, iters=100)\n",
        "    print(f\"Inference Time   : {metrics['inference_ms']:.4f} ms / sample\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}